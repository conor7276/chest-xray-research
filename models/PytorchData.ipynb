{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86ee9a09-ca9b-4bf9-ae86-b3eaf94385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2788f63-a0e4-4c2a-85f0-e9e1e9bf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/chest_xray_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e039d761-4b6a-4214-9fe8-faf0f0bd8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4adfdb-2853-4aa3-95b5-2570288068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d232810f-1a44-4e54-a88f-e1c1b63e249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(X_train.values, dtype= torch.int16)\n",
    "test_tensor = torch.tensor(y_test.values, dtype = torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70aa980e-2123-4874-92bf-ef4f8adf008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4,   3,   3,  ...,   0,   0,   0],\n",
       "        [  0,  13,  45,  ...,   0,   0,   0],\n",
       "        [ 20,  20,  20,  ...,  20,  20,  20],\n",
       "        ...,\n",
       "        [ 92, 102, 179,  ...,  23,  24,  23],\n",
       "        [ 52,  65,  73,  ...,   0,   0,   0],\n",
       "        [ 10,  23,  62,  ...,  24,  26,  37]], dtype=torch.int16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e094d1d-9a61-45cc-8dd3-c1f0f1746601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c49a7599-ef06-4655-8b5d-78c651de9a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
       "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
       "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
       "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
       "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
       "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
       "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65cbe147-35b5-441a-bc5e-7e30d7ecaf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7185290-c5b5-438f-8e20-57ed271a82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f65d7306-3fa2-4e85-a7e5-89f310b2c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "class ChestxrayImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=pil_transform_tensor, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c28a7f39-af79-4efa-85ef-d2d81f2a670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\train_images\\\\train_images\"\n",
    "annotations_file = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\labels_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7b3e41b-5e7b-4bb1-b55f-bb14ed6e4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_dataset = ChestxrayImageDataset(annotations_file=annotations_file, img_dir=img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e82414f7-45a0-4de4-8ff1-40c31793533b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: ChestxrayImageDataset.__getitem__ at line 31 (1478 times), pil_transform_tensor at line 2 (1478 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 27\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 27\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m read_image(img_path)\n\u001b[0;32m     29\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4002\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;124;03mQuickly retrieve single value at passed column and index.\u001b[39;00m\n\u001b[0;32m   3985\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m`self.columns._index_as_unique`; Caller is responsible for checking.\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m takeable:\n\u001b[1;32m-> 4002\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[0;32m   4005\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(col)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   3801\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[0;32m   3803\u001b[0m col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39miget(i)\n\u001b[1;32m-> 3804\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_box_col_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_as_cached(label, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4392\u001b[0m, in \u001b[0;36mDataFrame._box_col_values\u001b[1;34m(self, values, loc)\u001b[0m\n\u001b[0;32m   4390\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n\u001b[0;32m   4391\u001b[0m \u001b[38;5;66;03m# We get index=self.index bc values is a SingleDataManager\u001b[39;00m\n\u001b[1;32m-> 4392\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4393\u001b[0m obj\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   4394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:656\u001b[0m, in \u001b[0;36mDataFrame._constructor_sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced \u001b[38;5;129;01mis\u001b[39;00m Series:\n\u001b[1;32m--> 656\u001b[0m         ser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m         ser\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:652\u001b[0m, in \u001b[0;36mDataFrame._sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:352\u001b[0m, in \u001b[0;36mNDFrame._from_mgr\u001b[1;34m(cls, mgr, axes)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mConstruct a new object of this type from a Manager object and axes.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03min the event that axes are refactored out of the Manager objects.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:279\u001b[0m, in \u001b[0;36mNDFrame.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_item_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mFlags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\flags.py:55\u001b[0m, in \u001b[0;36mFlags.__init__\u001b[1;34m(self, obj, allows_duplicate_labels)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: NDFrame, \u001b[38;5;241m*\u001b[39m, allows_duplicate_labels: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "chest_dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3ee334-1623-4826-ba0d-3eeea74380c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chest_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PIL method\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chest_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# PIL method\n",
    "test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "test_img[0][0].shape # access image in tensor\n",
    "test_img = test_img[0][0] # image\n",
    "test_img.shape\n",
    "test_img = np.array(test_img) # turn to np array\n",
    "test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "img_gray = ImageOps.grayscale(test_pil_img)\n",
    "image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "image_resized\n",
    "image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "image_resized_tensor\n",
    "image_resized_tensor[0]\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(image_resized_tensor[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d48af6-a227-49ee-a61b-e9da4dd9cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_transform_tensor(tensor_img):\n",
    "    # test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "    # test_img[0][0].shape # access image in tensor\n",
    "    test_img = test_img[0][0] # image\n",
    "    # test_img.shape\n",
    "    test_img = np.array(test_img) # turn to np array\n",
    "    test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "    img_gray = ImageOps.grayscale(test_pil_img)\n",
    "    image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "    # image_resized\n",
    "    image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "    # image_resized_tensor\n",
    "    return image_resized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90489696-f947-45c7-889d-36e40ad57ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd7c0-49f0-4a63-8e8e-b7c96c4b22f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41e763af-9992-4e3a-a3ba-1c4e094b76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        # # get all pixel columns\n",
    "        # selected_features = list(df.columns)\n",
    "        # selected_features.remove(\"file_name\")\n",
    "        # selected_features.remove(\"class_id\")\n",
    "        \n",
    "        # # Set train and target\n",
    "        # X = df[selected_features]\n",
    "        # y = df['class_id']\n",
    "        \n",
    "        # # Get training and test data\n",
    "        # X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "        # # Turn into np arrays \n",
    "        # X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)\n",
    "        # y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)\n",
    "\n",
    "        # # Turn into PyTorch Sensors\n",
    "        # X_train_torch = torch.from_numpy(X_train_np)\n",
    "        # y_test_torch = torch.from_numpy(y_test_np)\n",
    "\n",
    "        # self.X = X_train_torch\n",
    "        # self.y = y_test_torch\n",
    "\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "        \n",
    "        # if train == True:\n",
    "        #     X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # else:\n",
    "        #     y_train, y_test = y.iloc[train_indices], y[test_indices]\n",
    "        \n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "        \n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806b5a98-38bd-4686-a278-2eebafc6f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c970b6d6-43d0-4684-8aec-53b837d2175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>file_name</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002194571005371555.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002972834724824498.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>img_1004160693662088646.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1011159426506457600.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>img_1014387197248837154.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       4       3       3       2       1       0       0       0       0   \n",
       "1       0      13      45      71      86      95     109     103      94   \n",
       "2      20      20      20      20      20      20      20      20      20   \n",
       "3       1      11      27      49      83     107     118     123     131   \n",
       "4      47      77     117      79     140      64      55      62      69   \n",
       "\n",
       "   pixel9  ...  pixel4088  pixel4089  pixel4090  pixel4091  pixel4092  \\\n",
       "0       2  ...        108         41          0          0          0   \n",
       "1      96  ...         99         33          0          0          0   \n",
       "2      20  ...         53         13         21         20         20   \n",
       "3     140  ...        149        135        112         99         65   \n",
       "4      67  ...        136        117         68         51         36   \n",
       "\n",
       "   pixel4093  pixel4094  pixel4095                    file_name  class_id  \n",
       "0          0          0          0  img_1002194571005371555.jpg         1  \n",
       "1          0          0          0  img_1002972834724824498.jpg         1  \n",
       "2         20         20         20  img_1004160693662088646.jpg         0  \n",
       "3         32         10          0  img_1011159426506457600.jpg         2  \n",
       "4         17         27         36  img_1014387197248837154.jpg         1  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92afaa7d-6d2e-46c3-9327-0525fb0f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc7aa4c-c39e-4980-b6a8-249f95ee57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc06ae7f-1f50-49eb-92c2-45e497cc69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aecfb10-6bd3-4650-b007-98aa3d65ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4205, 4096)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a2bc951-718f-48d4-9767-37167241002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973af36e-595e-4928-87a0-76cb0ca4c448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b2233d2-0c1e-4c96-b0c9-963c69ff1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f91ad91a-8944-44d8-ac76-42f5e6c54d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016b5ac1-df21-421f-a9d4-f1a28eb25f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_torch = torch.from_numpy(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "786db0ea-9c4f-4129-8ac0-1c329e446406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505144b4-974f-4bad-95fc-4d2cc8c335c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chest_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChestXRayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mChestXRayDataset.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m flattened_img \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[row,\u001b[38;5;241m0\u001b[39m:columns_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;66;03m# skip class_id\u001b[39;00m\n\u001b[0;32m     70\u001b[0m class_id \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[row,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# get class_id\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m matrix_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# turn flattened image back into 2d image\u001b[39;00m\n\u001b[0;32m     74\u001b[0m df_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_X, [matrix_img],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# append image\u001b[39;00m\n\u001b[0;32m     75\u001b[0m df_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_y, class_id) \u001b[38;5;66;03m# append class\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1469\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(a, new_shape)\u001b[0m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_shape, (\u001b[38;5;28mint\u001b[39m, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m   1467\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m (new_shape,)\n\u001b[1;32m-> 1469\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1471\u001b[0m new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim_length \u001b[38;5;129;01min\u001b[39;00m new_shape:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1874\u001b[0m, in \u001b[0;36mravel\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chest_dataset = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefda4a-750c-4ebb-abec-18eac9b8ba52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chest_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec14a3-6092-4b2b-9ff0-9f79f4a67f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_dataset, batch_size = 100, shuffle = True, num_workers = 2)\n",
    "test_dataloader = DataLoader(dataset = chest_dataset, batch_size = 5000, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffac4dd3-67bc-41f6-8cf1-dce746cfbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn pandas df into np array\n",
    "#dimensions [[64,64],1] [[64x64 image], class_id]\n",
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a4f2a11-76e9-4d06-b917-9bfd52a4a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = np.array([],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0b5a10-cc3c-4b9d-8d5a-e7e536dfa170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7ccb023-e4e1-4472-ba97-486e562b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"file_name\"]) # unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7da4b67f-ac74-4d27-a5fe-c6f976fcb81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4087</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>203</td>\n",
       "      <td>186</td>\n",
       "      <td>167</td>\n",
       "      <td>63</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>77</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>103</td>\n",
       "      <td>115</td>\n",
       "      <td>126</td>\n",
       "      <td>121</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>161</td>\n",
       "      <td>124</td>\n",
       "      <td>60</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>92</td>\n",
       "      <td>102</td>\n",
       "      <td>179</td>\n",
       "      <td>120</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>52</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>80</td>\n",
       "      <td>104</td>\n",
       "      <td>133</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>171</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "      <td>148</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4672 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          4       3       3       2       1       0       0       0       0   \n",
       "1          0      13      45      71      86      95     109     103      94   \n",
       "2         20      20      20      20      20      20      20      20      20   \n",
       "3          1      11      27      49      83     107     118     123     131   \n",
       "4         47      77     117      79     140      64      55      62      69   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4667     203     186     167      63      30      35      41      52      51   \n",
       "4668      88      90      93     103     115     126     121     126     128   \n",
       "4669      92     102     179     120      73      73      83      83      74   \n",
       "4670      52      65      73      81      87      80     104     133     175   \n",
       "4671      10      23      62      57      67      68      70      72      73   \n",
       "\n",
       "      pixel9  ...  pixel4087  pixel4088  pixel4089  pixel4090  pixel4091  \\\n",
       "0          2  ...        135        108         41          0          0   \n",
       "1         96  ...        129         99         33          0          0   \n",
       "2         20  ...        110         53         13         21         20   \n",
       "3        140  ...        130        149        135        112         99   \n",
       "4         67  ...        138        136        117         68         51   \n",
       "...      ...  ...        ...        ...        ...        ...        ...   \n",
       "4667      50  ...         86         95         77         48         17   \n",
       "4668     129  ...        192        186        177        161        124   \n",
       "4669      67  ...         99         51         10         17         22   \n",
       "4670     203  ...        179        170        171        132         93   \n",
       "4671      75  ...        188        185        178        148         83   \n",
       "\n",
       "      pixel4092  pixel4093  pixel4094  pixel4095  class_id  \n",
       "0             0          0          0          0         1  \n",
       "1             0          0          0          0         1  \n",
       "2            20         20         20         20         0  \n",
       "3            65         32         10          0         2  \n",
       "4            36         17         27         36         1  \n",
       "...         ...        ...        ...        ...       ...  \n",
       "4667          9         15         16         18         1  \n",
       "4668         60         36         48         51         1  \n",
       "4669         24         23         24         23         1  \n",
       "4670         40          0          0          0         0  \n",
       "4671         31         24         26         37         1  \n",
       "\n",
       "[4672 rows x 4097 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c17dce9-a425-403e-867c-adf20d9c0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "print(columns_len)\n",
    "\n",
    "# We have to get the first image in order to initalize array\n",
    "first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "first_img = np.resize(first_img,(64,64))\n",
    "\n",
    "# Get first class to initalize array\n",
    "second = df.loc[0,\"class_id\"]\n",
    "\n",
    "df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "\n",
    "df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "\n",
    "for row in range(1,len(df)):\n",
    "    \n",
    "    flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "    class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "    matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "\n",
    "    df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "    df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b3a5a02-bb63-4490-a80c-ba8cf806d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672, 64, 64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5514a5b-deae-45ea-bdcd-00324500665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ea1074cf-f930-4feb-a96b-ecf5deee30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = df_X[train_indices,:], df_X[test_indices,:] \n",
    "y_train, y_test = df_y[train_indices], df_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4564017d-bc0a-42a0-9a30-fac2198503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4205, 64, 64) (467, 64, 64) (4205,) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a63a626-341a-4d9a-8b75-6431c2d8b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2dff90c7-6af5-4026-b7d4-2aab34a49d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b25b160c-c768-4cf5-b31d-9795f85fc3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467, 64, 64])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch = torch.from_numpy(X_test)\n",
    "X_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7787a457-e0af-441c-8231-bf81a432da35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
       "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
       "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
       "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
       "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
       "         ...,\n",
       "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
       "\n",
       "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
       "         [  0,   2,  26,  ...,  39,  14,   1],\n",
       "         [  0,   0,  21,  ...,  47,  14,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
       "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
       "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
       "         ...,\n",
       "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
       "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
       "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
       "\n",
       "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
       "         [  1,   0,  33,  ...,   0,   0,   0],\n",
       "         [  5,   0,  29,  ...,   0,   0,   4],\n",
       "         ...,\n",
       "         [  5,   0,   0,  ...,   0,   0,   5],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
       "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
       "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
       "         ...,\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 30,  28,  26,  ...,  22,  23,  24]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54ed148-3e32-488f-aefe-a8785092d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "#Testing New Class\n",
    "chest_xray = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee9d896-bce6-44c0-8d1d-f64f7786dc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chest_xray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c4e734-c1e1-4b7d-bc62-37bf62edbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_train = chest_xray[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4db8bd-9e46-43dc-8a52-93fb1e9fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4205, 64, 64]) (4205,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_train[0].shape, chest_xray_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27acb10a-314c-4427-befb-919b7667d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_test = chest_xray[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7506351d-6cbf-4653-9edc-40b3d43c6e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64]) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_test[0].shape, chest_xray_test[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "444ce630-36cd-4e9f-99ed-48bf5398ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray_train, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b11acb0-437c-4fe3-8dee-9c6795b8da96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e54e007d90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f914543b-66dd-42b3-8fb2-e64377acce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataiter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d663e579-8c0f-4705-a35a-da047fecd376",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92075df2-ad1d-43d4-a591-3383e7e0db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8866d93a-61e1-4a8e-8039-ab21c11c5522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 65,  72,  86,  ...,  24,  19,  12],\n",
       "         [ 74,  86,  86,  ...,  29,  25,  22],\n",
       "         [ 78,  91,  94,  ...,  30,  28,  24],\n",
       "         ...,\n",
       "         [ 45,  43,  35,  ...,  44,  45,  46],\n",
       "         [ 44,  44,  33,  ...,  45,  46,  46],\n",
       "         [ 44,  43,  31,  ...,  45,  46,  46]],\n",
       "\n",
       "        [[ 48,  61,  59,  ...,  61,  50,  30],\n",
       "         [ 38,  58,  60,  ...,  57,  42,  19],\n",
       "         [ 25,  52,  62,  ...,  51,  40,  15],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   4,   0,   0],\n",
       "         [  0,   0,   0,  ...,   6,   0,   0],\n",
       "         [  0,   0,   0,  ...,   9,   0,   0]],\n",
       "\n",
       "        [[  8,  34,  51,  ..., 192, 195, 211],\n",
       "         [ 12,  48,  48,  ..., 186, 189, 203],\n",
       "         [ 39,  48,  49,  ..., 188, 191, 195],\n",
       "         ...,\n",
       "         [ 32,  30,  29,  ...,  28,  29,  33],\n",
       "         [ 32,  30,  29,  ...,  28,  28,  32],\n",
       "         [ 32,  30,  29,  ...,  28,  29,  31]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[122, 121, 141,  ...,   2,   6,  12],\n",
       "         [146, 129, 118,  ...,  20,  26,  31],\n",
       "         [ 87, 145, 141,  ...,  40,  45,  50],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 18,  34,  48,  ...,  85,  84,  89],\n",
       "         [ 31,  43,  57,  ...,  92, 100,  99],\n",
       "         [ 41,  58,  83,  ..., 109, 112, 104],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 55,  68,  85,  ...,  53,  52,  44],\n",
       "         [ 57,  64,  72,  ...,  56,  47,  44],\n",
       "         [ 59,  69,  79,  ...,  61,  51,  49],\n",
       "         ...,\n",
       "         [ 21,  14,  14,  ...,  14,  20,  26],\n",
       "         [ 19,  14,  16,  ...,  17,  16,  25],\n",
       "         [ 19,  13,  18,  ...,  22,  13,  26]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc5e629-1563-4141-82fa-3f5d44f7e4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,  17,  60,  ..., 121, 120,  93],\n",
      "         [  0,  15,  57,  ..., 117, 111,  91],\n",
      "         [  0,  13,  54,  ..., 118, 108,  87],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 19,  19,  15,  ...,  48,  45,  36],\n",
      "         [ 20,  19,   3,  ...,  43,  45,  33],\n",
      "         [ 16,  10,  16,  ...,  45,  37,  90],\n",
      "         ...,\n",
      "         [ 30,  29,  29,  ...,  28,  29,  30],\n",
      "         [ 30,  30,  29,  ...,  28,  29,  30],\n",
      "         [ 32,  29,  29,  ...,  29,  28,  32]],\n",
      "\n",
      "        [[ 40,  44,  44,  ...,  16,  23,  14],\n",
      "         [ 39,  43,  49,  ...,  14,  20,  14],\n",
      "         [ 41,  46,  55,  ...,  11,  16,  16],\n",
      "         ...,\n",
      "         [ 26,  25,  18,  ...,  27,  27,  28],\n",
      "         [ 26,  25,  17,  ...,  27,  27,  28],\n",
      "         [ 25,  24,  16,  ...,  27,  28,  28]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[117, 115, 122,  ..., 102, 114, 132],\n",
      "         [130, 123, 127,  ..., 110, 120, 128],\n",
      "         [121, 140, 134,  ..., 110, 120, 127],\n",
      "         ...,\n",
      "         [  3,  30,  58,  ...,  10,   0,   0],\n",
      "         [  3,  30,  61,  ...,  12,   0,   0],\n",
      "         [  5,  31,  62,  ...,  15,   0,   1]],\n",
      "\n",
      "        [[116,  88,  80,  ..., 147, 130, 120],\n",
      "         [ 97, 103, 115,  ..., 153, 151, 138],\n",
      "         [ 87,  92, 110,  ..., 145, 169, 153],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 34,  48,  56,  ...,  64,  47, 155],\n",
      "         [ 48,  59,  64,  ...,  79,  75, 172],\n",
      "         [ 48,  63,  71,  ...,  37,  41, 144],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ..., 117,  33,   1],\n",
      "         [  0,   0,   0,  ...,  99,  36,   2],\n",
      "         [  0,   0,   0,  ...,  89,  37,   3]]]) tensor([0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 2, 0, 2, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 0,\n",
      "        1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0,\n",
      "        0, 0, 2, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 2,\n",
      "        1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ef92e41-37c9-48cf-8b36-cefefac420ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "# Test if train = True and Train = False works\n",
    "chest_xray = ChestXRayDataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5829ecd4-a130-48bb-ae3e-035b03beac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffb4bc3b-54fe-4f16-a599-c46d8fe3d6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34639f73-ab63-4764-ab33-09445ee2f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray, batch_size = 100, shuffle = True)\n",
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ccd7686-f592-4d0d-a3e7-901bf8d981e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 32, 41,  ..., 83, 80, 76],\n",
       "        [32, 47, 58,  ..., 82, 75, 68],\n",
       "        [39, 57, 72,  ..., 80, 71, 59],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ..., 17,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 23,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 27,  0,  1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a93895ac-2cea-4424-a6af-db9453992995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 31,  40,  53,  ...,  18,  23,  23],\n",
      "         [ 35,  46,  60,  ...,  14,  16,  23],\n",
      "         [ 37,  54,  69,  ...,  32,  14,  11],\n",
      "         ...,\n",
      "         [ 19,  13,  18,  ...,  22,  23,  24],\n",
      "         [ 19,  13,  19,  ...,  23,  24,  25],\n",
      "         [ 18,  13,  19,  ...,  24,  27,  25]],\n",
      "\n",
      "        [[ 77,  83,  92,  ...,   0,   0,   0],\n",
      "         [128, 101,  91,  ...,  10,  18,  27],\n",
      "         [137, 150, 128,  ...,  45,  51,  57],\n",
      "         ...,\n",
      "         [  1,  25,  64,  ...,   2,   0,   0],\n",
      "         [  2,  26,  74,  ...,   3,   0,   0],\n",
      "         [  2,  28,  82,  ...,   5,   0,   0]],\n",
      "\n",
      "        [[  0,   0,   4,  ...,   0,   0,   0],\n",
      "         [  0,   0,   8,  ...,   0,   2,   0],\n",
      "         [  0,   0,  14,  ...,   0,   5,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 55,  76,  93,  ...,  78,  73,  54],\n",
      "         [ 52,  74,  89,  ...,  79,  69,  49],\n",
      "         [ 48,  72,  88,  ...,  85,  67,  45],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   9,   0,   1],\n",
      "         [  0,   1,   0,  ...,  11,   0,   1],\n",
      "         [  0,   1,   0,  ...,  12,   0,   1]],\n",
      "\n",
      "        [[ 18,  36,  61,  ...,  59,  44,  39],\n",
      "         [ 56,  62,  42,  ...,  41,  39,  44],\n",
      "         [ 41,  39,  45,  ...,  41,  38,  51],\n",
      "         ...,\n",
      "         [ 33,  31,  32,  ...,  13,  21,  21],\n",
      "         [ 33,  30,  31,  ...,  13,  21,  22],\n",
      "         [ 34,  30,  30,  ...,  13,  20,  22]],\n",
      "\n",
      "        [[ 88,  13,  26,  ..., 192, 204, 225],\n",
      "         [ 23,  17,  39,  ..., 193, 208, 224],\n",
      "         [ 17,  24,  41,  ..., 194, 208, 221],\n",
      "         ...,\n",
      "         [ 28,  25,  23,  ...,  25,  25,  30],\n",
      "         [ 27,  25,  23,  ...,  25,  25,  29],\n",
      "         [ 26,  25,  22,  ...,  25,  26,  27]]]) tensor([1, 0, 2, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 1,\n",
      "        2, 1, 0, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1,\n",
      "        0, 0, 2, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n",
      "        1, 0, 1, 1])\n",
      "torch.Size([100, 64, 64]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f3905e9-48d6-4a17-94c5-025d839a761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64])\n",
      "torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "chest_xray_validation = ChestXRayDataset(False)\n",
    "print(chest_xray_validation.X.shape)\n",
    "print(chest_xray_validation.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87771576-dccc-46d7-a20b-d6f6a9c9ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(dataset = chest_xray_validation, batch_size = 500)\n",
    "valid_dataiter = iter(validation_dataloader)\n",
    "valid_data = next(valid_dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f53f02b4-3a91-439d-81b5-152fcdf000e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
      "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
      "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
      "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
      "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
      "         ...,\n",
      "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
      "\n",
      "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
      "         [  0,   2,  26,  ...,  39,  14,   1],\n",
      "         [  0,   0,  21,  ...,  47,  14,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
      "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
      "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
      "         ...,\n",
      "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
      "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
      "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
      "\n",
      "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
      "         [  1,   0,  33,  ...,   0,   0,   0],\n",
      "         [  5,   0,  29,  ...,   0,   0,   4],\n",
      "         ...,\n",
      "         [  5,   0,   0,  ...,   0,   0,   5],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
      "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
      "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
      "         ...,\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 30,  28,  26,  ...,  22,  23,  24]]]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
      "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
      "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
      "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
      "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
      "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
      "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
      "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([467, 64, 64]) torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "for x,y in validation_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc203ee7-24b1-42f0-98d2-8d516c667f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN please work time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9331f960-9c7a-4a2c-94d1-44f9cc71ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Library\n",
    "import torch\n",
    "# PyTorch Neural Network\n",
    "import torch.nn as nn\n",
    "# Allows us to transform data\n",
    "import torchvision.transforms as transforms\n",
    "# Allows us to download the dataset\n",
    "import torchvision.datasets as dsets\n",
    "# Used to graph data and loss curves\n",
    "import matplotlib.pylab as plt\n",
    "# Allows us to use arrays to manipulate and store data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d2198d-716b-4ef1-8f2f-d1c7fd7ce1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the channels\n",
    "\n",
    "def plot_channels(W):\n",
    "    n_out = W.shape[0]\n",
    "    n_in = W.shape[1]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(n_out, n_in)\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "    out_index = 0\n",
    "    in_index = 0\n",
    "    \n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "        if in_index > n_in-1:\n",
    "            out_index = out_index + 1\n",
    "            in_index = 0\n",
    "        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index = in_index + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the parameters\n",
    "\n",
    "def plot_parameters(W, number_rows=1, name=\"\", i=0):\n",
    "    W = W.data[:, i, :, :]\n",
    "    n_filters = W.shape[0]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_filters:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.suptitle(name, fontsize=10)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the activations\n",
    "\n",
    "def plot_activations(A, number_rows=1, name=\"\", i=0):\n",
    "    A = A[0, :, :, :].detach().numpy()\n",
    "    n_activations = A.shape[0]\n",
    "    A_min = A.min().item()\n",
    "    A_max = A.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n",
    "    fig.subplots_adjust(hspace = 0.9)    \n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_activations:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1cd973b1-a617-4db0-a1f3-c355d5ab14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "\n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.float32) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.float32) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "            # df_X = df_X.astype(np.float32)\n",
    "            # df_y = df_y.astype(np.float32)\n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            # df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_X, df_y = df_X[train_indices], df_X[test_indices]\n",
    "            # df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            # df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_X, df_y = df_y[train_indices], df_y[test_indices] #y_train, y_test\n",
    "            # df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f96e4f2a-b3c8-4e10-a5d7-860c75440b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        # The reason we start with 1 channel is because we have a single black and white image\n",
    "        # Channel Width after this layer is 16\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        # Channel Wifth after this layer is 8\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Channel Width after this layer is 8\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        # Channel Width after this layer is 4\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n",
    "        # The output is a value for each class\n",
    "        # self.fc1 = nn.Linear(out_2 * 4 * 4, 3)\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 3)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    # Outputs result of each stage of the CNN, relu, and pooling layers\n",
    "    def activations(self, x):\n",
    "        # Outputs activation this is not necessary\n",
    "        z1 = self.cnn1(x)\n",
    "        a1 = torch.relu(z1)\n",
    "        out = self.maxpool1(a1)\n",
    "        \n",
    "        z2 = self.cnn2(out)\n",
    "        a2 = torch.relu(z2)\n",
    "        out1 = self.maxpool2(a2)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return z1, a1, z2, a2, out1,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "3162fdcb-166e-433a-9f41-d485b36f3e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN(out_1=16, out_2=32)\n",
    "plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\n",
    "plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "24b446dc-e5f0-4226-96c4-121c1440d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n",
      "4097\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChestXRayDataset(True)\n",
    "validation_dataset = ChestXRayDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "32a7d16b-672d-4fae-b0fe-b4d275d85ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4205, 64, 64]) (467, 64, 64) torch.Size([4205]) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.X.shape,train_dataset.y.shape,validation_dataset.X.shape,validation_dataset.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c8e55608-98ae-470b-82f9-4f12c3f857b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dataset.X = train_dataset.X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353a691-135f-4d43-85e8-a911b7147f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dat    = torch.utils.data.TensorDataset(torch.tensor(test_data_x).to(device), torch.tensor(test_data_y).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "45c70975-3591-4b4c-a208-9bca4d4a25c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1945876125.py, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[18], line 5\u001b[1;36m\u001b[0m\n\u001b[1;33m    validation_dataset.X.(torch.float32)\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# train_dataset.X.set_default_dtype(torch.float32)\n",
    "# train_dataset.X.float()\n",
    "# train_dataset.y.int()\n",
    "# validation_dataset.X.float()\n",
    "validation_dataset.X.(torch.float32)\n",
    "# validation_dataset.y.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf7d0270-1c1a-4646-a5c9-03a225ad7eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2ffe137c-755f-4b2d-9d19-9848c75d5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a criterion which will measure loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "# Create an optimizer that updates model parameters using the learning rate and gradient\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# Create a Data Loader for the training data with a batch size of 100 \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "# Create a Data Loader for the validation data with a batch size of 5000 \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd09b9ee-f883-4b1d-a6e6-edcd9d432fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 64, 64])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x8192 and 512x3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[63], line 53\u001b[0m\n\u001b[0;32m     50\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m N_test\n\u001b[0;32m     51\u001b[0m         accuracy_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m---> 53\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[63], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Measures the loss between prediction and acutal Y value\u001b[39;00m\n\u001b[0;32m     28\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(z, y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[54], line 30\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool2(x)\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x8192 and 512x3)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Number of times we want to train on the taining dataset\n",
    "n_epochs=3\n",
    "# List to keep track of cost and accuracy\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "# Size of the validation dataset\n",
    "N_test=len(validation_dataset)\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(n_epochs):\n",
    "    # Loops for each epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        # Keeps track of cost for each epoch\n",
    "        COST=0\n",
    "        # For each batch in train loader\n",
    "        for x, y in train_loader:\n",
    "            # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
    "            optimizer.zero_grad()\n",
    "            # Makes a prediction based on X value\n",
    "            x = torch.reshape(x,(100,1,64,64))\n",
    "            print(x.shape)\n",
    "            # break\n",
    "            z = model(x)\n",
    "            \n",
    "            # Measures the loss between prediction and acutal Y value\n",
    "            loss = criterion(z, y)\n",
    "            # Calculates the gradient value with respect to each weight and bias\n",
    "            loss.backward()\n",
    "            # Updates the weight and bias according to calculated gradient value\n",
    "            optimizer.step()\n",
    "            # Cumulates loss \n",
    "            COST+=loss.data\n",
    "        \n",
    "        # Saves cost of training data of epoch\n",
    "        cost_list.append(COST)\n",
    "        # Keeps track of correct predictions\n",
    "        correct=0\n",
    "        # Perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            # Makes a prediction\n",
    "            z = model(x_test)\n",
    "            # The class with the max value is the one we are predicting\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            # Checks if the prediction matches the actual value\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        \n",
    "        # Calcualtes accuracy and saves it\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f158610-53e1-4b3c-b3ea-8e65cd20cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f52abd-4bd4-4296-a30d-5adab9a17c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ced57-ebfc-492b-ac6b-869bff5d879f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211127-2ab7-4dea-8f8a-e062bda41662",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859b85e-06bc-4406-a349-bcb6709d579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a3d53-c1b6-4372-8461-49bd71bbad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83398d5d-85a1-4616-b659-0f976933b8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780ed74-c63e-4e5a-99dc-a8c8f51c9105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
