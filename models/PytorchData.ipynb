{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86ee9a09-ca9b-4bf9-ae86-b3eaf94385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2788f63-a0e4-4c2a-85f0-e9e1e9bf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/chest_xray_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e039d761-4b6a-4214-9fe8-faf0f0bd8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4adfdb-2853-4aa3-95b5-2570288068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d232810f-1a44-4e54-a88f-e1c1b63e249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(X_train.values, dtype= torch.int16)\n",
    "test_tensor = torch.tensor(y_test.values, dtype = torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70aa980e-2123-4874-92bf-ef4f8adf008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4,   3,   3,  ...,   0,   0,   0],\n",
       "        [  0,  13,  45,  ...,   0,   0,   0],\n",
       "        [ 20,  20,  20,  ...,  20,  20,  20],\n",
       "        ...,\n",
       "        [ 92, 102, 179,  ...,  23,  24,  23],\n",
       "        [ 52,  65,  73,  ...,   0,   0,   0],\n",
       "        [ 10,  23,  62,  ...,  24,  26,  37]], dtype=torch.int16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e094d1d-9a61-45cc-8dd3-c1f0f1746601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c49a7599-ef06-4655-8b5d-78c651de9a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
       "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
       "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
       "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
       "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
       "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
       "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65cbe147-35b5-441a-bc5e-7e30d7ecaf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7185290-c5b5-438f-8e20-57ed271a82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f65d7306-3fa2-4e85-a7e5-89f310b2c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "class ChestxrayImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=pil_transform_tensor, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c28a7f39-af79-4efa-85ef-d2d81f2a670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\train_images\\\\train_images\"\n",
    "annotations_file = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\labels_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7b3e41b-5e7b-4bb1-b55f-bb14ed6e4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_dataset = ChestxrayImageDataset(annotations_file=annotations_file, img_dir=img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e82414f7-45a0-4de4-8ff1-40c31793533b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: ChestxrayImageDataset.__getitem__ at line 31 (1478 times), pil_transform_tensor at line 2 (1478 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 27\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 27\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m read_image(img_path)\n\u001b[0;32m     29\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4002\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;124;03mQuickly retrieve single value at passed column and index.\u001b[39;00m\n\u001b[0;32m   3985\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m`self.columns._index_as_unique`; Caller is responsible for checking.\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m takeable:\n\u001b[1;32m-> 4002\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[0;32m   4005\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(col)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   3801\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[0;32m   3803\u001b[0m col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39miget(i)\n\u001b[1;32m-> 3804\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_box_col_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_as_cached(label, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4392\u001b[0m, in \u001b[0;36mDataFrame._box_col_values\u001b[1;34m(self, values, loc)\u001b[0m\n\u001b[0;32m   4390\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n\u001b[0;32m   4391\u001b[0m \u001b[38;5;66;03m# We get index=self.index bc values is a SingleDataManager\u001b[39;00m\n\u001b[1;32m-> 4392\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4393\u001b[0m obj\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   4394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:656\u001b[0m, in \u001b[0;36mDataFrame._constructor_sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced \u001b[38;5;129;01mis\u001b[39;00m Series:\n\u001b[1;32m--> 656\u001b[0m         ser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m         ser\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:652\u001b[0m, in \u001b[0;36mDataFrame._sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:352\u001b[0m, in \u001b[0;36mNDFrame._from_mgr\u001b[1;34m(cls, mgr, axes)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mConstruct a new object of this type from a Manager object and axes.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03min the event that axes are refactored out of the Manager objects.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:279\u001b[0m, in \u001b[0;36mNDFrame.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_item_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mFlags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\flags.py:55\u001b[0m, in \u001b[0;36mFlags.__init__\u001b[1;34m(self, obj, allows_duplicate_labels)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: NDFrame, \u001b[38;5;241m*\u001b[39m, allows_duplicate_labels: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "chest_dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3ee334-1623-4826-ba0d-3eeea74380c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chest_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PIL method\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chest_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# PIL method\n",
    "test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "test_img[0][0].shape # access image in tensor\n",
    "test_img = test_img[0][0] # image\n",
    "test_img.shape\n",
    "test_img = np.array(test_img) # turn to np array\n",
    "test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "img_gray = ImageOps.grayscale(test_pil_img)\n",
    "image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "image_resized\n",
    "image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "image_resized_tensor\n",
    "image_resized_tensor[0]\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(image_resized_tensor[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d48af6-a227-49ee-a61b-e9da4dd9cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_transform_tensor(tensor_img):\n",
    "    # test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "    # test_img[0][0].shape # access image in tensor\n",
    "    test_img = test_img[0][0] # image\n",
    "    # test_img.shape\n",
    "    test_img = np.array(test_img) # turn to np array\n",
    "    test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "    img_gray = ImageOps.grayscale(test_pil_img)\n",
    "    image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "    # image_resized\n",
    "    image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "    # image_resized_tensor\n",
    "    return image_resized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90489696-f947-45c7-889d-36e40ad57ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd7c0-49f0-4a63-8e8e-b7c96c4b22f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41e763af-9992-4e3a-a3ba-1c4e094b76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        # # get all pixel columns\n",
    "        # selected_features = list(df.columns)\n",
    "        # selected_features.remove(\"file_name\")\n",
    "        # selected_features.remove(\"class_id\")\n",
    "        \n",
    "        # # Set train and target\n",
    "        # X = df[selected_features]\n",
    "        # y = df['class_id']\n",
    "        \n",
    "        # # Get training and test data\n",
    "        # X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "        # # Turn into np arrays \n",
    "        # X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)\n",
    "        # y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)\n",
    "\n",
    "        # # Turn into PyTorch Sensors\n",
    "        # X_train_torch = torch.from_numpy(X_train_np)\n",
    "        # y_test_torch = torch.from_numpy(y_test_np)\n",
    "\n",
    "        # self.X = X_train_torch\n",
    "        # self.y = y_test_torch\n",
    "\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "        \n",
    "        # if train == True:\n",
    "        #     X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # else:\n",
    "        #     y_train, y_test = y.iloc[train_indices], y[test_indices]\n",
    "        \n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "        \n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806b5a98-38bd-4686-a278-2eebafc6f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c970b6d6-43d0-4684-8aec-53b837d2175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>file_name</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002194571005371555.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002972834724824498.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>img_1004160693662088646.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1011159426506457600.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>img_1014387197248837154.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       4       3       3       2       1       0       0       0       0   \n",
       "1       0      13      45      71      86      95     109     103      94   \n",
       "2      20      20      20      20      20      20      20      20      20   \n",
       "3       1      11      27      49      83     107     118     123     131   \n",
       "4      47      77     117      79     140      64      55      62      69   \n",
       "\n",
       "   pixel9  ...  pixel4088  pixel4089  pixel4090  pixel4091  pixel4092  \\\n",
       "0       2  ...        108         41          0          0          0   \n",
       "1      96  ...         99         33          0          0          0   \n",
       "2      20  ...         53         13         21         20         20   \n",
       "3     140  ...        149        135        112         99         65   \n",
       "4      67  ...        136        117         68         51         36   \n",
       "\n",
       "   pixel4093  pixel4094  pixel4095                    file_name  class_id  \n",
       "0          0          0          0  img_1002194571005371555.jpg         1  \n",
       "1          0          0          0  img_1002972834724824498.jpg         1  \n",
       "2         20         20         20  img_1004160693662088646.jpg         0  \n",
       "3         32         10          0  img_1011159426506457600.jpg         2  \n",
       "4         17         27         36  img_1014387197248837154.jpg         1  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92afaa7d-6d2e-46c3-9327-0525fb0f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc7aa4c-c39e-4980-b6a8-249f95ee57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc06ae7f-1f50-49eb-92c2-45e497cc69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aecfb10-6bd3-4650-b007-98aa3d65ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4205, 4096)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a2bc951-718f-48d4-9767-37167241002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973af36e-595e-4928-87a0-76cb0ca4c448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b2233d2-0c1e-4c96-b0c9-963c69ff1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f91ad91a-8944-44d8-ac76-42f5e6c54d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016b5ac1-df21-421f-a9d4-f1a28eb25f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_torch = torch.from_numpy(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "786db0ea-9c4f-4129-8ac0-1c329e446406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505144b4-974f-4bad-95fc-4d2cc8c335c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chest_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChestXRayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mChestXRayDataset.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m flattened_img \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[row,\u001b[38;5;241m0\u001b[39m:columns_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;66;03m# skip class_id\u001b[39;00m\n\u001b[0;32m     70\u001b[0m class_id \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[row,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# get class_id\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m matrix_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# turn flattened image back into 2d image\u001b[39;00m\n\u001b[0;32m     74\u001b[0m df_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_X, [matrix_img],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# append image\u001b[39;00m\n\u001b[0;32m     75\u001b[0m df_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_y, class_id) \u001b[38;5;66;03m# append class\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1469\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(a, new_shape)\u001b[0m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_shape, (\u001b[38;5;28mint\u001b[39m, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m   1467\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m (new_shape,)\n\u001b[1;32m-> 1469\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1471\u001b[0m new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim_length \u001b[38;5;129;01min\u001b[39;00m new_shape:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1874\u001b[0m, in \u001b[0;36mravel\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chest_dataset = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefda4a-750c-4ebb-abec-18eac9b8ba52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chest_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec14a3-6092-4b2b-9ff0-9f79f4a67f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_dataset, batch_size = 100, shuffle = True, num_workers = 2)\n",
    "test_dataloader = DataLoader(dataset = chest_dataset, batch_size = 5000, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffac4dd3-67bc-41f6-8cf1-dce746cfbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn pandas df into np array\n",
    "#dimensions [[64,64],1] [[64x64 image], class_id]\n",
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a4f2a11-76e9-4d06-b917-9bfd52a4a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = np.array([],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0b5a10-cc3c-4b9d-8d5a-e7e536dfa170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7ccb023-e4e1-4472-ba97-486e562b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"file_name\"]) # unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7da4b67f-ac74-4d27-a5fe-c6f976fcb81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4087</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>203</td>\n",
       "      <td>186</td>\n",
       "      <td>167</td>\n",
       "      <td>63</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>77</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>103</td>\n",
       "      <td>115</td>\n",
       "      <td>126</td>\n",
       "      <td>121</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>161</td>\n",
       "      <td>124</td>\n",
       "      <td>60</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>92</td>\n",
       "      <td>102</td>\n",
       "      <td>179</td>\n",
       "      <td>120</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>52</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>80</td>\n",
       "      <td>104</td>\n",
       "      <td>133</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>171</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "      <td>148</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4672 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          4       3       3       2       1       0       0       0       0   \n",
       "1          0      13      45      71      86      95     109     103      94   \n",
       "2         20      20      20      20      20      20      20      20      20   \n",
       "3          1      11      27      49      83     107     118     123     131   \n",
       "4         47      77     117      79     140      64      55      62      69   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4667     203     186     167      63      30      35      41      52      51   \n",
       "4668      88      90      93     103     115     126     121     126     128   \n",
       "4669      92     102     179     120      73      73      83      83      74   \n",
       "4670      52      65      73      81      87      80     104     133     175   \n",
       "4671      10      23      62      57      67      68      70      72      73   \n",
       "\n",
       "      pixel9  ...  pixel4087  pixel4088  pixel4089  pixel4090  pixel4091  \\\n",
       "0          2  ...        135        108         41          0          0   \n",
       "1         96  ...        129         99         33          0          0   \n",
       "2         20  ...        110         53         13         21         20   \n",
       "3        140  ...        130        149        135        112         99   \n",
       "4         67  ...        138        136        117         68         51   \n",
       "...      ...  ...        ...        ...        ...        ...        ...   \n",
       "4667      50  ...         86         95         77         48         17   \n",
       "4668     129  ...        192        186        177        161        124   \n",
       "4669      67  ...         99         51         10         17         22   \n",
       "4670     203  ...        179        170        171        132         93   \n",
       "4671      75  ...        188        185        178        148         83   \n",
       "\n",
       "      pixel4092  pixel4093  pixel4094  pixel4095  class_id  \n",
       "0             0          0          0          0         1  \n",
       "1             0          0          0          0         1  \n",
       "2            20         20         20         20         0  \n",
       "3            65         32         10          0         2  \n",
       "4            36         17         27         36         1  \n",
       "...         ...        ...        ...        ...       ...  \n",
       "4667          9         15         16         18         1  \n",
       "4668         60         36         48         51         1  \n",
       "4669         24         23         24         23         1  \n",
       "4670         40          0          0          0         0  \n",
       "4671         31         24         26         37         1  \n",
       "\n",
       "[4672 rows x 4097 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c17dce9-a425-403e-867c-adf20d9c0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "print(columns_len)\n",
    "\n",
    "# We have to get the first image in order to initalize array\n",
    "first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "first_img = np.resize(first_img,(64,64))\n",
    "\n",
    "# Get first class to initalize array\n",
    "second = df.loc[0,\"class_id\"]\n",
    "\n",
    "df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "\n",
    "df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "\n",
    "for row in range(1,len(df)):\n",
    "    \n",
    "    flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "    class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "    matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "\n",
    "    df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "    df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b3a5a02-bb63-4490-a80c-ba8cf806d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672, 64, 64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5514a5b-deae-45ea-bdcd-00324500665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ea1074cf-f930-4feb-a96b-ecf5deee30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = df_X[train_indices,:], df_X[test_indices,:] \n",
    "y_train, y_test = df_y[train_indices], df_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4564017d-bc0a-42a0-9a30-fac2198503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4205, 64, 64) (467, 64, 64) (4205,) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a63a626-341a-4d9a-8b75-6431c2d8b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2dff90c7-6af5-4026-b7d4-2aab34a49d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b25b160c-c768-4cf5-b31d-9795f85fc3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467, 64, 64])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch = torch.from_numpy(X_test)\n",
    "X_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7787a457-e0af-441c-8231-bf81a432da35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
       "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
       "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
       "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
       "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
       "         ...,\n",
       "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
       "\n",
       "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
       "         [  0,   2,  26,  ...,  39,  14,   1],\n",
       "         [  0,   0,  21,  ...,  47,  14,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
       "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
       "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
       "         ...,\n",
       "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
       "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
       "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
       "\n",
       "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
       "         [  1,   0,  33,  ...,   0,   0,   0],\n",
       "         [  5,   0,  29,  ...,   0,   0,   4],\n",
       "         ...,\n",
       "         [  5,   0,   0,  ...,   0,   0,   5],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
       "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
       "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
       "         ...,\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 30,  28,  26,  ...,  22,  23,  24]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54ed148-3e32-488f-aefe-a8785092d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "#Testing New Class\n",
    "chest_xray = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee9d896-bce6-44c0-8d1d-f64f7786dc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chest_xray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c4e734-c1e1-4b7d-bc62-37bf62edbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_train = chest_xray[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4db8bd-9e46-43dc-8a52-93fb1e9fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4205, 64, 64]) (4205,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_train[0].shape, chest_xray_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27acb10a-314c-4427-befb-919b7667d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_test = chest_xray[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7506351d-6cbf-4653-9edc-40b3d43c6e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64]) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_test[0].shape, chest_xray_test[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "444ce630-36cd-4e9f-99ed-48bf5398ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray_train, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b11acb0-437c-4fe3-8dee-9c6795b8da96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e54e007d90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f914543b-66dd-42b3-8fb2-e64377acce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataiter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d663e579-8c0f-4705-a35a-da047fecd376",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92075df2-ad1d-43d4-a591-3383e7e0db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8866d93a-61e1-4a8e-8039-ab21c11c5522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 65,  72,  86,  ...,  24,  19,  12],\n",
       "         [ 74,  86,  86,  ...,  29,  25,  22],\n",
       "         [ 78,  91,  94,  ...,  30,  28,  24],\n",
       "         ...,\n",
       "         [ 45,  43,  35,  ...,  44,  45,  46],\n",
       "         [ 44,  44,  33,  ...,  45,  46,  46],\n",
       "         [ 44,  43,  31,  ...,  45,  46,  46]],\n",
       "\n",
       "        [[ 48,  61,  59,  ...,  61,  50,  30],\n",
       "         [ 38,  58,  60,  ...,  57,  42,  19],\n",
       "         [ 25,  52,  62,  ...,  51,  40,  15],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   4,   0,   0],\n",
       "         [  0,   0,   0,  ...,   6,   0,   0],\n",
       "         [  0,   0,   0,  ...,   9,   0,   0]],\n",
       "\n",
       "        [[  8,  34,  51,  ..., 192, 195, 211],\n",
       "         [ 12,  48,  48,  ..., 186, 189, 203],\n",
       "         [ 39,  48,  49,  ..., 188, 191, 195],\n",
       "         ...,\n",
       "         [ 32,  30,  29,  ...,  28,  29,  33],\n",
       "         [ 32,  30,  29,  ...,  28,  28,  32],\n",
       "         [ 32,  30,  29,  ...,  28,  29,  31]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[122, 121, 141,  ...,   2,   6,  12],\n",
       "         [146, 129, 118,  ...,  20,  26,  31],\n",
       "         [ 87, 145, 141,  ...,  40,  45,  50],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 18,  34,  48,  ...,  85,  84,  89],\n",
       "         [ 31,  43,  57,  ...,  92, 100,  99],\n",
       "         [ 41,  58,  83,  ..., 109, 112, 104],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 55,  68,  85,  ...,  53,  52,  44],\n",
       "         [ 57,  64,  72,  ...,  56,  47,  44],\n",
       "         [ 59,  69,  79,  ...,  61,  51,  49],\n",
       "         ...,\n",
       "         [ 21,  14,  14,  ...,  14,  20,  26],\n",
       "         [ 19,  14,  16,  ...,  17,  16,  25],\n",
       "         [ 19,  13,  18,  ...,  22,  13,  26]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc5e629-1563-4141-82fa-3f5d44f7e4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,  17,  60,  ..., 121, 120,  93],\n",
      "         [  0,  15,  57,  ..., 117, 111,  91],\n",
      "         [  0,  13,  54,  ..., 118, 108,  87],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 19,  19,  15,  ...,  48,  45,  36],\n",
      "         [ 20,  19,   3,  ...,  43,  45,  33],\n",
      "         [ 16,  10,  16,  ...,  45,  37,  90],\n",
      "         ...,\n",
      "         [ 30,  29,  29,  ...,  28,  29,  30],\n",
      "         [ 30,  30,  29,  ...,  28,  29,  30],\n",
      "         [ 32,  29,  29,  ...,  29,  28,  32]],\n",
      "\n",
      "        [[ 40,  44,  44,  ...,  16,  23,  14],\n",
      "         [ 39,  43,  49,  ...,  14,  20,  14],\n",
      "         [ 41,  46,  55,  ...,  11,  16,  16],\n",
      "         ...,\n",
      "         [ 26,  25,  18,  ...,  27,  27,  28],\n",
      "         [ 26,  25,  17,  ...,  27,  27,  28],\n",
      "         [ 25,  24,  16,  ...,  27,  28,  28]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[117, 115, 122,  ..., 102, 114, 132],\n",
      "         [130, 123, 127,  ..., 110, 120, 128],\n",
      "         [121, 140, 134,  ..., 110, 120, 127],\n",
      "         ...,\n",
      "         [  3,  30,  58,  ...,  10,   0,   0],\n",
      "         [  3,  30,  61,  ...,  12,   0,   0],\n",
      "         [  5,  31,  62,  ...,  15,   0,   1]],\n",
      "\n",
      "        [[116,  88,  80,  ..., 147, 130, 120],\n",
      "         [ 97, 103, 115,  ..., 153, 151, 138],\n",
      "         [ 87,  92, 110,  ..., 145, 169, 153],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 34,  48,  56,  ...,  64,  47, 155],\n",
      "         [ 48,  59,  64,  ...,  79,  75, 172],\n",
      "         [ 48,  63,  71,  ...,  37,  41, 144],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ..., 117,  33,   1],\n",
      "         [  0,   0,   0,  ...,  99,  36,   2],\n",
      "         [  0,   0,   0,  ...,  89,  37,   3]]]) tensor([0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 2, 0, 2, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 0,\n",
      "        1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0,\n",
      "        0, 0, 2, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 2,\n",
      "        1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ef92e41-37c9-48cf-8b36-cefefac420ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "# Test if train = True and Train = False works\n",
    "chest_xray = ChestXRayDataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5829ecd4-a130-48bb-ae3e-035b03beac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffb4bc3b-54fe-4f16-a599-c46d8fe3d6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34639f73-ab63-4764-ab33-09445ee2f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray, batch_size = 100, shuffle = True)\n",
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ccd7686-f592-4d0d-a3e7-901bf8d981e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 32, 41,  ..., 83, 80, 76],\n",
       "        [32, 47, 58,  ..., 82, 75, 68],\n",
       "        [39, 57, 72,  ..., 80, 71, 59],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ..., 17,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 23,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 27,  0,  1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a93895ac-2cea-4424-a6af-db9453992995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 31,  40,  53,  ...,  18,  23,  23],\n",
      "         [ 35,  46,  60,  ...,  14,  16,  23],\n",
      "         [ 37,  54,  69,  ...,  32,  14,  11],\n",
      "         ...,\n",
      "         [ 19,  13,  18,  ...,  22,  23,  24],\n",
      "         [ 19,  13,  19,  ...,  23,  24,  25],\n",
      "         [ 18,  13,  19,  ...,  24,  27,  25]],\n",
      "\n",
      "        [[ 77,  83,  92,  ...,   0,   0,   0],\n",
      "         [128, 101,  91,  ...,  10,  18,  27],\n",
      "         [137, 150, 128,  ...,  45,  51,  57],\n",
      "         ...,\n",
      "         [  1,  25,  64,  ...,   2,   0,   0],\n",
      "         [  2,  26,  74,  ...,   3,   0,   0],\n",
      "         [  2,  28,  82,  ...,   5,   0,   0]],\n",
      "\n",
      "        [[  0,   0,   4,  ...,   0,   0,   0],\n",
      "         [  0,   0,   8,  ...,   0,   2,   0],\n",
      "         [  0,   0,  14,  ...,   0,   5,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 55,  76,  93,  ...,  78,  73,  54],\n",
      "         [ 52,  74,  89,  ...,  79,  69,  49],\n",
      "         [ 48,  72,  88,  ...,  85,  67,  45],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   9,   0,   1],\n",
      "         [  0,   1,   0,  ...,  11,   0,   1],\n",
      "         [  0,   1,   0,  ...,  12,   0,   1]],\n",
      "\n",
      "        [[ 18,  36,  61,  ...,  59,  44,  39],\n",
      "         [ 56,  62,  42,  ...,  41,  39,  44],\n",
      "         [ 41,  39,  45,  ...,  41,  38,  51],\n",
      "         ...,\n",
      "         [ 33,  31,  32,  ...,  13,  21,  21],\n",
      "         [ 33,  30,  31,  ...,  13,  21,  22],\n",
      "         [ 34,  30,  30,  ...,  13,  20,  22]],\n",
      "\n",
      "        [[ 88,  13,  26,  ..., 192, 204, 225],\n",
      "         [ 23,  17,  39,  ..., 193, 208, 224],\n",
      "         [ 17,  24,  41,  ..., 194, 208, 221],\n",
      "         ...,\n",
      "         [ 28,  25,  23,  ...,  25,  25,  30],\n",
      "         [ 27,  25,  23,  ...,  25,  25,  29],\n",
      "         [ 26,  25,  22,  ...,  25,  26,  27]]]) tensor([1, 0, 2, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 1,\n",
      "        2, 1, 0, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1,\n",
      "        0, 0, 2, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n",
      "        1, 0, 1, 1])\n",
      "torch.Size([100, 64, 64]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f3905e9-48d6-4a17-94c5-025d839a761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64])\n",
      "torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "chest_xray_validation = ChestXRayDataset(False)\n",
    "print(chest_xray_validation.X.shape)\n",
    "print(chest_xray_validation.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87771576-dccc-46d7-a20b-d6f6a9c9ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(dataset = chest_xray_validation, batch_size = 500)\n",
    "valid_dataiter = iter(validation_dataloader)\n",
    "valid_data = next(valid_dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f53f02b4-3a91-439d-81b5-152fcdf000e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
      "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
      "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
      "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
      "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
      "         ...,\n",
      "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
      "\n",
      "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
      "         [  0,   2,  26,  ...,  39,  14,   1],\n",
      "         [  0,   0,  21,  ...,  47,  14,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
      "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
      "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
      "         ...,\n",
      "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
      "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
      "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
      "\n",
      "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
      "         [  1,   0,  33,  ...,   0,   0,   0],\n",
      "         [  5,   0,  29,  ...,   0,   0,   4],\n",
      "         ...,\n",
      "         [  5,   0,   0,  ...,   0,   0,   5],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
      "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
      "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
      "         ...,\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 30,  28,  26,  ...,  22,  23,  24]]]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
      "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
      "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
      "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
      "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
      "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
      "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
      "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([467, 64, 64]) torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "for x,y in validation_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc203ee7-24b1-42f0-98d2-8d516c667f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN please work time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "9331f960-9c7a-4a2c-94d1-44f9cc71ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Library\n",
    "import torch\n",
    "# PyTorch Neural Network\n",
    "import torch.nn as nn\n",
    "# Allows us to transform data\n",
    "import torchvision.transforms as transforms\n",
    "# Allows us to download the dataset\n",
    "import torchvision.datasets as dsets\n",
    "# Used to graph data and loss curves\n",
    "import matplotlib.pylab as plt\n",
    "# Allows us to use arrays to manipulate and store data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "19d2198d-716b-4ef1-8f2f-d1c7fd7ce1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the channels\n",
    "\n",
    "def plot_channels(W):\n",
    "    n_out = W.shape[0]\n",
    "    n_in = W.shape[1]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(n_out, n_in)\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "    out_index = 0\n",
    "    in_index = 0\n",
    "    \n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "        if in_index > n_in-1:\n",
    "            out_index = out_index + 1\n",
    "            in_index = 0\n",
    "        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index = in_index + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the parameters\n",
    "\n",
    "def plot_parameters(W, number_rows=1, name=\"\", i=0):\n",
    "    W = W.data[:, i, :, :]\n",
    "    n_filters = W.shape[0]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_filters:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.suptitle(name, fontsize=10)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the activations\n",
    "\n",
    "def plot_activations(A, number_rows=1, name=\"\", i=0):\n",
    "    A = A[0, :, :, :].detach().numpy()\n",
    "    n_activations = A.shape[0]\n",
    "    A_min = A.min().item()\n",
    "    A_max = A.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n",
    "    fig.subplots_adjust(hspace = 0.9)    \n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_activations:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "1cd973b1-a617-4db0-a1f3-c355d5ab14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "\n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "            df_X = df_X.astype(np.float32)\n",
    "            df_y = df_y.astype(np.float32)\n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f96e4f2a-b3c8-4e10-a5d7-860c75440b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        # The reason we start with 1 channel is because we have a single black and white image\n",
    "        # Channel Width after this layer is 16\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        # Channel Wifth after this layer is 8\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Channel Width after this layer is 8\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        # Channel Width after this layer is 4\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n",
    "        # The output is a value for each class\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    # Outputs result of each stage of the CNN, relu, and pooling layers\n",
    "    def activations(self, x):\n",
    "        # Outputs activation this is not necessary\n",
    "        z1 = self.cnn1(x)\n",
    "        a1 = torch.relu(z1)\n",
    "        out = self.maxpool1(a1)\n",
    "        \n",
    "        z2 = self.cnn2(out)\n",
    "        a2 = torch.relu(z2)\n",
    "        out1 = self.maxpool2(a2)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return z1, a1, z2, a2, out1,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "3162fdcb-166e-433a-9f41-d485b36f3e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN(out_1=16, out_2=32)\n",
    "plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\n",
    "plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "24b446dc-e5f0-4226-96c4-121c1440d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n",
      "4097\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChestXRayDataset(True)\n",
    "validation_dataset = ChestXRayDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353a691-135f-4d43-85e8-a911b7147f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat    = torch.utils.data.TensorDataset(torch.tensor(test_data_x).to(device), torch.tensor(test_data_y).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "45c70975-3591-4b4c-a208-9bca4d4a25c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'set_default_dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_default_dtype\u001b[49m(torch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# train_dataset.X.float()\u001b[39;00m\n\u001b[0;32m      3\u001b[0m train_dataset\u001b[38;5;241m.\u001b[39my\u001b[38;5;241m.\u001b[39mint()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'set_default_dtype'"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset.X.set_default_dtype(torch.float32)\n",
    "# train_dataset.X.float()\n",
    "train_dataset.y.int()\n",
    "# validation_dataset.X.float()\n",
    "validation_dataset.X.set_default_dtype(torch.float32)\n",
    "validation_dataset.y.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bf7d0270-1c1a-4646-a5c9-03a225ad7eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 55.,  63.,  69.,  ..., 153., 165., 138.],\n",
       "         [ 62.,  72.,  81.,  ..., 156., 135., 117.],\n",
       "         [ 60.,  80.,  89.,  ..., 139., 132., 116.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.,  ...,   0.,   1.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   1.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 38.,  51.,  56.,  ...,  66.,  46.,  30.],\n",
       "         [ 37.,  50.,  55.,  ...,  65.,  46.,  30.],\n",
       "         [ 35.,  51.,  58.,  ...,  63.,  46.,  26.],\n",
       "         ...,\n",
       "         [ 15.,  12.,  47.,  ...,  22.,  25.,  24.],\n",
       "         [ 15.,  12.,  46.,  ...,  23.,  24.,  24.],\n",
       "         [ 15.,  12.,  46.,  ...,  23.,  24.,  25.]],\n",
       "\n",
       "        [[  0.,   4.,  31.,  ...,  31.,  19.,   6.],\n",
       "         [  0.,   2.,  26.,  ...,  39.,  14.,   1.],\n",
       "         [  0.,   0.,  21.,  ...,  47.,  14.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62.,  61.,  62.,  ..., 131., 157., 179.],\n",
       "         [ 57.,  57.,  57.,  ..., 125., 165., 178.],\n",
       "         [ 50.,  54.,  48.,  ..., 119., 163., 179.],\n",
       "         ...,\n",
       "         [ 84., 125., 147.,  ..., 162., 165., 143.],\n",
       "         [ 90., 125., 145.,  ..., 156., 162., 145.],\n",
       "         [ 94., 126., 144.,  ..., 150., 160., 147.]],\n",
       "\n",
       "        [[  2.,   0.,  41.,  ...,   0.,   0.,   0.],\n",
       "         [  1.,   0.,  33.,  ...,   0.,   0.,   0.],\n",
       "         [  5.,   0.,  29.,  ...,   0.,   0.,   4.],\n",
       "         ...,\n",
       "         [  5.,   0.,   0.,  ...,   0.,   0.,   5.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   1.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 71.,  62.,  63.,  ...,  42.,  42.,  47.],\n",
       "         [ 76.,  61.,  49.,  ...,  46.,  43.,  49.],\n",
       "         [ 62.,  56.,  67.,  ...,  57.,  49.,  50.],\n",
       "         ...,\n",
       "         [ 29.,  28.,  26.,  ...,  22.,  23.,  23.],\n",
       "         [ 29.,  28.,  26.,  ...,  22.,  23.,  23.],\n",
       "         [ 30.,  28.,  26.,  ...,  22.,  23.,  24.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "2ffe137c-755f-4b2d-9d19-9848c75d5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a criterion which will measure loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "# Create an optimizer that updates model parameters using the learning rate and gradient\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# Create a Data Loader for the training data with a batch size of 100 \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=100)\n",
    "# Create a Data Loader for the validation data with a batch size of 5000 \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "dd09b9ee-f883-4b1d-a6e6-edcd9d432fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  4.,   3.,   3.,  ...,  19.,   0.,   1.],\n",
      "         [  3.,   3.,   3.,  ...,   4.,   0.,   0.],\n",
      "         [  3.,   3.,   2.,  ...,   0.,   1.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,  13.,  45.,  ...,  89.,  86.,  83.],\n",
      "         [  4.,  30.,  62.,  ..., 101.,  99.,  96.],\n",
      "         [ 17.,  50.,  69.,  ..., 107., 101.,  99.],\n",
      "         ...,\n",
      "         [  0.,   0.,   1.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[ 20.,  20.,  20.,  ...,  20.,  20.,  20.],\n",
      "         [ 19.,  20.,  20.,  ...,  20.,  20.,  20.],\n",
      "         [ 19.,  20.,  20.,  ...,  19.,  19.,  20.],\n",
      "         ...,\n",
      "         [  3.,  20.,  20.,  ...,  20.,  20.,  20.],\n",
      "         [  2.,  20.,  21.,  ...,  20.,  20.,  20.],\n",
      "         [  2.,  20.,  21.,  ...,  20.,  20.,  20.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  9.,   5.,  12.,  ...,   4.,  40.,  45.],\n",
      "         [  5.,   8.,  40.,  ...,   8.,  12.,  59.],\n",
      "         [  5.,  30.,  49.,  ...,  11.,   6.,  25.],\n",
      "         ...,\n",
      "         [  6.,  40.,  64.,  ...,  16.,  16.,  16.],\n",
      "         [  7.,  41.,  62.,  ...,  15.,  16.,  17.],\n",
      "         [  7.,  40.,  62.,  ...,  16.,  17.,  17.]],\n",
      "\n",
      "        [[ 41.,  53.,  65.,  ...,  80.,  69.,  56.],\n",
      "         [ 35.,  49.,  61.,  ...,  77.,  64.,  50.],\n",
      "         [ 30.,  45.,  59.,  ...,  74.,  59.,  46.],\n",
      "         ...,\n",
      "         [  0.,   0.,   1.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   1.,  ...,   1.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   1.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,  30.,  ...,  14.,  33.,  45.],\n",
      "         [  6.,   7.,  46.,  ...,  12.,  18.,  27.],\n",
      "         [ 18.,  21.,  46.,  ...,  12.,  22.,  30.],\n",
      "         ...,\n",
      "         [  7.,   5.,   2.,  ...,  16.,  22.,  30.],\n",
      "         [ 11.,   5.,   2.,  ...,  16.,  21.,  34.],\n",
      "         [  8.,   5.,   1.,  ...,  16.,  22.,  32.]]], dtype=torch.float64)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Input type (double) and bias type (float) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[130], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m N_test\n\u001b[0;32m     50\u001b[0m         accuracy_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[130], line 42\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Perform a prediction on the validation  data  \u001b[39;00m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x_test, y_test \u001b[38;5;129;01min\u001b[39;00m validation_loader:\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# Makes a prediction\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;66;03m# The class with the max value is the one we are predicting\u001b[39;00m\n\u001b[0;32m     44\u001b[0m     _, yhat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(z\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[109], line 23\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcnn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(x)\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (double) and bias type (float) should be the same"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Number of times we want to train on the taining dataset\n",
    "n_epochs=3\n",
    "# List to keep track of cost and accuracy\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "# Size of the validation dataset\n",
    "N_test=len(validation_dataset)\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(n_epochs):\n",
    "    # Loops for each epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        # Keeps track of cost for each epoch\n",
    "        COST=0\n",
    "        # For each batch in train loader\n",
    "        for x, y in train_loader:\n",
    "            # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
    "            optimizer.zero_grad()\n",
    "            # Makes a prediction based on X value\n",
    "\n",
    "            print(x)\n",
    "            break\n",
    "            z = model(x)\n",
    "            # Measures the loss between prediction and acutal Y value\n",
    "            loss = criterion(z, y)\n",
    "            # Calculates the gradient value with respect to each weight and bias\n",
    "            loss.backward()\n",
    "            # Updates the weight and bias according to calculated gradient value\n",
    "            optimizer.step()\n",
    "            # Cumulates loss \n",
    "            COST+=loss.data\n",
    "        \n",
    "        # Saves cost of training data of epoch\n",
    "        cost_list.append(COST)\n",
    "        # Keeps track of correct predictions\n",
    "        correct=0\n",
    "        # Perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            # Makes a prediction\n",
    "            z = model(x_test)\n",
    "            # The class with the max value is the one we are predicting\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            # Checks if the prediction matches the actual value\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        \n",
    "        # Calcualtes accuracy and saves it\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f158610-53e1-4b3c-b3ea-8e65cd20cbe9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f52abd-4bd4-4296-a30d-5adab9a17c4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369ced57-ebfc-492b-ac6b-869bff5d879f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb211127-2ab7-4dea-8f8a-e062bda41662",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2859b85e-06bc-4406-a349-bcb6709d579e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7a3d53-c1b6-4372-8461-49bd71bbad3d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83398d5d-85a1-4616-b659-0f976933b8df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780ed74-c63e-4e5a-99dc-a8c8f51c9105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
