{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "86ee9a09-ca9b-4bf9-ae86-b3eaf94385c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a2788f63-a0e4-4c2a-85f0-e9e1e9bf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/chest_xray_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e039d761-4b6a-4214-9fe8-faf0f0bd8e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7d4adfdb-2853-4aa3-95b5-2570288068c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d232810f-1a44-4e54-a88f-e1c1b63e249f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tensor = torch.tensor(X_train.values, dtype= torch.int16)\n",
    "test_tensor = torch.tensor(y_test.values, dtype = torch.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "70aa980e-2123-4874-92bf-ef4f8adf008f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  4,   3,   3,  ...,   0,   0,   0],\n",
       "        [  0,  13,  45,  ...,   0,   0,   0],\n",
       "        [ 20,  20,  20,  ...,  20,  20,  20],\n",
       "        ...,\n",
       "        [ 92, 102, 179,  ...,  23,  24,  23],\n",
       "        [ 52,  65,  73,  ...,   0,   0,   0],\n",
       "        [ 10,  23,  62,  ...,  24,  26,  37]], dtype=torch.int16)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1e094d1d-9a61-45cc-8dd3-c1f0f1746601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c49a7599-ef06-4655-8b5d-78c651de9a32",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
       "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
       "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
       "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
       "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
       "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
       "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
       "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
       "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
       "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
       "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
       "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2], dtype=torch.int16)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "65cbe147-35b5-441a-bc5e-7e30d7ecaf08",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7185290-c5b5-438f-8e20-57ed271a82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f65d7306-3fa2-4e85-a7e5-89f310b2c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torchvision.transforms.functional import to_pil_image, pil_to_tensor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from PIL import Image\n",
    "from PIL import ImageOps\n",
    "\n",
    "class ChestxrayImageDataset(Dataset):\n",
    "    def __init__(self, annotations_file, img_dir, transform=pil_transform_tensor, target_transform=None):\n",
    "        self.img_labels = pd.read_csv(annotations_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])\n",
    "        image = read_image(img_path)\n",
    "        label = self.img_labels.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        if self.target_transform:\n",
    "            label = self.target_transform(label)\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c28a7f39-af79-4efa-85ef-d2d81f2a670e",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_dir = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\train_images\\\\train_images\"\n",
    "annotations_file = \"C:\\\\Users\\\\conor\\\\OneDrive\\\\Desktop\\\\school\\\\Data Science Minor Independent Study\\\\chest-xray-research\\\\archive\\\\labels_train.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a7b3e41b-5e7b-4bb1-b55f-bb14ed6e4f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_dataset = ChestxrayImageDataset(annotations_file=annotations_file, img_dir=img_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e82414f7-45a0-4de4-8ff1-40c31793533b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RecursionError",
     "evalue": "maximum recursion depth exceeded while calling a Python object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRecursionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping similar frames: ChestxrayImageDataset.__getitem__ at line 31 (1478 times), pil_transform_tensor at line 2 (1478 times)]\u001b[0m\n",
      "Cell \u001b[1;32mIn[51], line 31\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     29\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 31\u001b[0m     image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform:\n\u001b[0;32m     33\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_transform(label)\n",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m, in \u001b[0;36mpil_transform_tensor\u001b[1;34m(tensor_img)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpil_transform_tensor\u001b[39m(tensor_img):\n\u001b[1;32m----> 2\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getitem__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;66;03m# test_img[0][0].shape # access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m     test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[51], line 27\u001b[0m, in \u001b[0;36mChestxrayImageDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx):\n\u001b[1;32m---> 27\u001b[0m     img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_dir, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimg_labels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     28\u001b[0m     image \u001b[38;5;241m=\u001b[39m read_image(img_path)\n\u001b[0;32m     29\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_labels\u001b[38;5;241m.\u001b[39miloc[idx, \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\indexing.py:1146\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(com\u001b[38;5;241m.\u001b[39mapply_if_callable(x, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m key)\n\u001b[0;32m   1145\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_scalar_access(key):\n\u001b[1;32m-> 1146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtakeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_takeable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_tuple(key)\n\u001b[0;32m   1148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1149\u001b[0m     \u001b[38;5;66;03m# we by definition only have the 0th axis\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4002\u001b[0m, in \u001b[0;36mDataFrame._get_value\u001b[1;34m(self, index, col, takeable)\u001b[0m\n\u001b[0;32m   3983\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   3984\u001b[0m \u001b[38;5;124;03mQuickly retrieve single value at passed column and index.\u001b[39;00m\n\u001b[0;32m   3985\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3999\u001b[0m \u001b[38;5;124;03m`self.columns._index_as_unique`; Caller is responsible for checking.\u001b[39;00m\n\u001b[0;32m   4000\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4001\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m takeable:\n\u001b[1;32m-> 4002\u001b[0m     series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4003\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m series\u001b[38;5;241m.\u001b[39m_values[index]\n\u001b[0;32m   4005\u001b[0m series \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_item_cache(col)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:3804\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[1;34m(self, i, axis)\u001b[0m\n\u001b[0;32m   3801\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[0;32m   3803\u001b[0m col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39miget(i)\n\u001b[1;32m-> 3804\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_box_col_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3806\u001b[0m \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_as_cached(label, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:4392\u001b[0m, in \u001b[0;36mDataFrame._box_col_values\u001b[1;34m(self, values, loc)\u001b[0m\n\u001b[0;32m   4390\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[loc]\n\u001b[0;32m   4391\u001b[0m \u001b[38;5;66;03m# We get index=self.index bc values is a SingleDataManager\u001b[39;00m\n\u001b[1;32m-> 4392\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4393\u001b[0m obj\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m   4394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:656\u001b[0m, in \u001b[0;36mDataFrame._constructor_sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    654\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constructor_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes):\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced \u001b[38;5;129;01mis\u001b[39;00m Series:\n\u001b[1;32m--> 656\u001b[0m         ser \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sliced_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    657\u001b[0m         ser\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# caller is responsible for setting real name\u001b[39;00m\n\u001b[0;32m    658\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ser\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\frame.py:652\u001b[0m, in \u001b[0;36mDataFrame._sliced_from_mgr\u001b[1;34m(self, mgr, axes)\u001b[0m\n\u001b[0;32m    651\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_sliced_from_mgr\u001b[39m(\u001b[38;5;28mself\u001b[39m, mgr, axes) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series:\n\u001b[1;32m--> 652\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:352\u001b[0m, in \u001b[0;36mNDFrame._from_mgr\u001b[1;34m(cls, mgr, axes)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    338\u001b[0m \u001b[38;5;124;03mConstruct a new object of this type from a Manager object and axes.\u001b[39;00m\n\u001b[0;32m    339\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[38;5;124;03min the event that axes are refactored out of the Manager objects.\u001b[39;00m\n\u001b[0;32m    350\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    351\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__new__\u001b[39m(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m--> 352\u001b[0m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmgr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\generic.py:279\u001b[0m, in \u001b[0;36mNDFrame.__init__\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_item_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[0;32m    278\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_attrs\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m--> 279\u001b[0m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__setattr__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_flags\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mFlags\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallows_duplicate_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\pandas\\core\\flags.py:55\u001b[0m, in \u001b[0;36mFlags.__init__\u001b[1;34m(self, obj, allows_duplicate_labels)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, obj: NDFrame, \u001b[38;5;241m*\u001b[39m, allows_duplicate_labels: \u001b[38;5;28mbool\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allows_duplicate_labels \u001b[38;5;241m=\u001b[39m allows_duplicate_labels\n\u001b[1;32m---> 55\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj \u001b[38;5;241m=\u001b[39m \u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRecursionError\u001b[0m: maximum recursion depth exceeded while calling a Python object"
     ]
    }
   ],
   "source": [
    "chest_dataset.__getitem__(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae3ee334-1623-4826-ba0d-3eeea74380c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chest_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# PIL method\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test_img \u001b[38;5;241m=\u001b[39m \u001b[43mchest_dataset\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;241m2\u001b[39m) \u001b[38;5;66;03m# get second image in labels list\u001b[39;00m\n\u001b[0;32m      3\u001b[0m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;66;03m# access image in tensor\u001b[39;00m\n\u001b[0;32m      4\u001b[0m test_img \u001b[38;5;241m=\u001b[39m test_img[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;66;03m# image\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'chest_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "# PIL method\n",
    "test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "test_img[0][0].shape # access image in tensor\n",
    "test_img = test_img[0][0] # image\n",
    "test_img.shape\n",
    "test_img = np.array(test_img) # turn to np array\n",
    "test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "img_gray = ImageOps.grayscale(test_pil_img)\n",
    "image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "image_resized\n",
    "image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "image_resized_tensor\n",
    "image_resized_tensor[0]\n",
    "# plt.figure(figsize=(10,10))\n",
    "# plt.imshow(image_resized_tensor[0])\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "88d48af6-a227-49ee-a61b-e9da4dd9cb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pil_transform_tensor(tensor_img):\n",
    "    # test_img = chest_dataset.__getitem__(2) # get second image in labels list\n",
    "    # test_img[0][0].shape # access image in tensor\n",
    "    test_img = test_img[0][0] # image\n",
    "    # test_img.shape\n",
    "    test_img = np.array(test_img) # turn to np array\n",
    "    test_pil_img = to_pil_image(test_img) # turn to pil and rescale\n",
    "    img_gray = ImageOps.grayscale(test_pil_img)\n",
    "    image_resized = img_gray.resize((64,64), Image.LANCZOS)\n",
    "    # image_resized\n",
    "    image_resized_tensor = pil_to_tensor(image_resized) # turn back into tensor\n",
    "    # image_resized_tensor\n",
    "    return image_resized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "90489696-f947-45c7-889d-36e40ad57ae6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 64])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0bd7c0-49f0-4a63-8e8e-b7c96c4b22f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "41e763af-9992-4e3a-a3ba-1c4e094b76cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        # # get all pixel columns\n",
    "        # selected_features = list(df.columns)\n",
    "        # selected_features.remove(\"file_name\")\n",
    "        # selected_features.remove(\"class_id\")\n",
    "        \n",
    "        # # Set train and target\n",
    "        # X = df[selected_features]\n",
    "        # y = df['class_id']\n",
    "        \n",
    "        # # Get training and test data\n",
    "        # X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # y_train, y_test = y[train_indices], y[test_indices]\n",
    "\n",
    "        # # Turn into np arrays \n",
    "        # X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)\n",
    "        # y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)\n",
    "\n",
    "        # # Turn into PyTorch Sensors\n",
    "        # X_train_torch = torch.from_numpy(X_train_np)\n",
    "        # y_test_torch = torch.from_numpy(y_test_np)\n",
    "\n",
    "        # self.X = X_train_torch\n",
    "        # self.y = y_test_torch\n",
    "\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "        \n",
    "        # if train == True:\n",
    "        #     X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "        # else:\n",
    "        #     y_train, y_test = y.iloc[train_indices], y[test_indices]\n",
    "        \n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "        \n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "806b5a98-38bd-4686-a278-2eebafc6f42f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c970b6d6-43d0-4684-8aec-53b837d2175d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>file_name</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002194571005371555.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1002972834724824498.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>img_1004160693662088646.jpg</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>img_1011159426506457600.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>img_1014387197248837154.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4098 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0       4       3       3       2       1       0       0       0       0   \n",
       "1       0      13      45      71      86      95     109     103      94   \n",
       "2      20      20      20      20      20      20      20      20      20   \n",
       "3       1      11      27      49      83     107     118     123     131   \n",
       "4      47      77     117      79     140      64      55      62      69   \n",
       "\n",
       "   pixel9  ...  pixel4088  pixel4089  pixel4090  pixel4091  pixel4092  \\\n",
       "0       2  ...        108         41          0          0          0   \n",
       "1      96  ...         99         33          0          0          0   \n",
       "2      20  ...         53         13         21         20         20   \n",
       "3     140  ...        149        135        112         99         65   \n",
       "4      67  ...        136        117         68         51         36   \n",
       "\n",
       "   pixel4093  pixel4094  pixel4095                    file_name  class_id  \n",
       "0          0          0          0  img_1002194571005371555.jpg         1  \n",
       "1          0          0          0  img_1002972834724824498.jpg         1  \n",
       "2         20         20         20  img_1004160693662088646.jpg         0  \n",
       "3         32         10          0  img_1011159426506457600.jpg         2  \n",
       "4         17         27         36  img_1014387197248837154.jpg         1  \n",
       "\n",
       "[5 rows x 4098 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92afaa7d-6d2e-46c3-9327-0525fb0f07e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "#Load Static testing and training data\n",
    "with open('static_data/train.pickle','rb') as handle:\n",
    "    train_indices = pickle.load(handle)\n",
    "\n",
    "with open('static_data/test.pickle','rb') as handle:\n",
    "    test_indices = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc7aa4c-c39e-4980-b6a8-249f95ee57c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all pixel columns\n",
    "selected_features = list(df.columns)\n",
    "selected_features.remove(\"file_name\")\n",
    "selected_features.remove(\"class_id\")\n",
    "\n",
    "# Set train and target\n",
    "X = df[selected_features]\n",
    "y = df['class_id']\n",
    "\n",
    "# Get training and test data\n",
    "X_train, X_test = X.iloc[train_indices,:], X.iloc[test_indices,:] \n",
    "y_train, y_test = y[train_indices], y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc06ae7f-1f50-49eb-92c2-45e497cc69b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_np = X_train.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4aecfb10-6bd3-4650-b007-98aa3d65ff3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4205, 4096)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7a2bc951-718f-48d4-9767-37167241002d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_np = y_test.to_numpy(dtype=np.int16, copy = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "973af36e-595e-4928-87a0-76cb0ca4c448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(467,)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b2233d2-0c1e-4c96-b0c9-963c69ff1c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f91ad91a-8944-44d8-ac76-42f5e6c54d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 4096])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "016b5ac1-df21-421f-a9d4-f1a28eb25f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_torch = torch.from_numpy(y_test_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "786db0ea-9c4f-4129-8ac0-1c329e446406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505144b4-974f-4bad-95fc-4d2cc8c335c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m chest_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mChestXRayDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 71\u001b[0m, in \u001b[0;36mChestXRayDataset.__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m flattened_img \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39miloc[row,\u001b[38;5;241m0\u001b[39m:columns_len\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto_numpy() \u001b[38;5;66;03m# skip class_id\u001b[39;00m\n\u001b[0;32m     70\u001b[0m class_id \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mloc[row,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;66;03m# get class_id\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m matrix_img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflattened_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# turn flattened image back into 2d image\u001b[39;00m\n\u001b[0;32m     74\u001b[0m df_X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_X, [matrix_img],axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;66;03m# append image\u001b[39;00m\n\u001b[0;32m     75\u001b[0m df_y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(df_y, class_id) \u001b[38;5;66;03m# append class\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1469\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(a, new_shape)\u001b[0m\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_shape, (\u001b[38;5;28mint\u001b[39m, nt\u001b[38;5;241m.\u001b[39minteger)):\n\u001b[0;32m   1467\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m (new_shape,)\n\u001b[1;32m-> 1469\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1471\u001b[0m new_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dim_length \u001b[38;5;129;01min\u001b[39;00m new_shape:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1874\u001b[0m, in \u001b[0;36mravel\u001b[1;34m(a, order)\u001b[0m\n\u001b[0;32m   1872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m asarray(a)\u001b[38;5;241m.\u001b[39mravel(order\u001b[38;5;241m=\u001b[39morder)\n\u001b[0;32m   1873\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43masanyarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mravel\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chest_dataset = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefda4a-750c-4ebb-abec-18eac9b8ba52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chest_dataset.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ec14a3-6092-4b2b-9ff0-9f79f4a67f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_dataset, batch_size = 100, shuffle = True, num_workers = 2)\n",
    "test_dataloader = DataLoader(dataset = chest_dataset, batch_size = 5000, num_workers = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ffac4dd3-67bc-41f6-8cf1-dce746cfbf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Turn pandas df into np array\n",
    "#dimensions [[64,64],1] [[64x64 image], class_id]\n",
    "df = pd.read_csv('data/chest_xray_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8a4f2a11-76e9-4d06-b917-9bfd52a4a1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_np = np.array([],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4b0b5a10-cc3c-4b9d-8d5a-e7e536dfa170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], dtype=int16)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7ccb023-e4e1-4472-ba97-486e562b0b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = [\"file_name\"]) # unnecessary column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7da4b67f-ac74-4d27-a5fe-c6f976fcb81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pixel0</th>\n",
       "      <th>pixel1</th>\n",
       "      <th>pixel2</th>\n",
       "      <th>pixel3</th>\n",
       "      <th>pixel4</th>\n",
       "      <th>pixel5</th>\n",
       "      <th>pixel6</th>\n",
       "      <th>pixel7</th>\n",
       "      <th>pixel8</th>\n",
       "      <th>pixel9</th>\n",
       "      <th>...</th>\n",
       "      <th>pixel4087</th>\n",
       "      <th>pixel4088</th>\n",
       "      <th>pixel4089</th>\n",
       "      <th>pixel4090</th>\n",
       "      <th>pixel4091</th>\n",
       "      <th>pixel4092</th>\n",
       "      <th>pixel4093</th>\n",
       "      <th>pixel4094</th>\n",
       "      <th>pixel4095</th>\n",
       "      <th>class_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>135</td>\n",
       "      <td>108</td>\n",
       "      <td>41</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>45</td>\n",
       "      <td>71</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>109</td>\n",
       "      <td>103</td>\n",
       "      <td>94</td>\n",
       "      <td>96</td>\n",
       "      <td>...</td>\n",
       "      <td>129</td>\n",
       "      <td>99</td>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>...</td>\n",
       "      <td>110</td>\n",
       "      <td>53</td>\n",
       "      <td>13</td>\n",
       "      <td>21</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>27</td>\n",
       "      <td>49</td>\n",
       "      <td>83</td>\n",
       "      <td>107</td>\n",
       "      <td>118</td>\n",
       "      <td>123</td>\n",
       "      <td>131</td>\n",
       "      <td>140</td>\n",
       "      <td>...</td>\n",
       "      <td>130</td>\n",
       "      <td>149</td>\n",
       "      <td>135</td>\n",
       "      <td>112</td>\n",
       "      <td>99</td>\n",
       "      <td>65</td>\n",
       "      <td>32</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "      <td>77</td>\n",
       "      <td>117</td>\n",
       "      <td>79</td>\n",
       "      <td>140</td>\n",
       "      <td>64</td>\n",
       "      <td>55</td>\n",
       "      <td>62</td>\n",
       "      <td>69</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>138</td>\n",
       "      <td>136</td>\n",
       "      <td>117</td>\n",
       "      <td>68</td>\n",
       "      <td>51</td>\n",
       "      <td>36</td>\n",
       "      <td>17</td>\n",
       "      <td>27</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4667</th>\n",
       "      <td>203</td>\n",
       "      <td>186</td>\n",
       "      <td>167</td>\n",
       "      <td>63</td>\n",
       "      <td>30</td>\n",
       "      <td>35</td>\n",
       "      <td>41</td>\n",
       "      <td>52</td>\n",
       "      <td>51</td>\n",
       "      <td>50</td>\n",
       "      <td>...</td>\n",
       "      <td>86</td>\n",
       "      <td>95</td>\n",
       "      <td>77</td>\n",
       "      <td>48</td>\n",
       "      <td>17</td>\n",
       "      <td>9</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4668</th>\n",
       "      <td>88</td>\n",
       "      <td>90</td>\n",
       "      <td>93</td>\n",
       "      <td>103</td>\n",
       "      <td>115</td>\n",
       "      <td>126</td>\n",
       "      <td>121</td>\n",
       "      <td>126</td>\n",
       "      <td>128</td>\n",
       "      <td>129</td>\n",
       "      <td>...</td>\n",
       "      <td>192</td>\n",
       "      <td>186</td>\n",
       "      <td>177</td>\n",
       "      <td>161</td>\n",
       "      <td>124</td>\n",
       "      <td>60</td>\n",
       "      <td>36</td>\n",
       "      <td>48</td>\n",
       "      <td>51</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4669</th>\n",
       "      <td>92</td>\n",
       "      <td>102</td>\n",
       "      <td>179</td>\n",
       "      <td>120</td>\n",
       "      <td>73</td>\n",
       "      <td>73</td>\n",
       "      <td>83</td>\n",
       "      <td>83</td>\n",
       "      <td>74</td>\n",
       "      <td>67</td>\n",
       "      <td>...</td>\n",
       "      <td>99</td>\n",
       "      <td>51</td>\n",
       "      <td>10</td>\n",
       "      <td>17</td>\n",
       "      <td>22</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>24</td>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4670</th>\n",
       "      <td>52</td>\n",
       "      <td>65</td>\n",
       "      <td>73</td>\n",
       "      <td>81</td>\n",
       "      <td>87</td>\n",
       "      <td>80</td>\n",
       "      <td>104</td>\n",
       "      <td>133</td>\n",
       "      <td>175</td>\n",
       "      <td>203</td>\n",
       "      <td>...</td>\n",
       "      <td>179</td>\n",
       "      <td>170</td>\n",
       "      <td>171</td>\n",
       "      <td>132</td>\n",
       "      <td>93</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4671</th>\n",
       "      <td>10</td>\n",
       "      <td>23</td>\n",
       "      <td>62</td>\n",
       "      <td>57</td>\n",
       "      <td>67</td>\n",
       "      <td>68</td>\n",
       "      <td>70</td>\n",
       "      <td>72</td>\n",
       "      <td>73</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>188</td>\n",
       "      <td>185</td>\n",
       "      <td>178</td>\n",
       "      <td>148</td>\n",
       "      <td>83</td>\n",
       "      <td>31</td>\n",
       "      <td>24</td>\n",
       "      <td>26</td>\n",
       "      <td>37</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4672 rows × 4097 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  pixel8  \\\n",
       "0          4       3       3       2       1       0       0       0       0   \n",
       "1          0      13      45      71      86      95     109     103      94   \n",
       "2         20      20      20      20      20      20      20      20      20   \n",
       "3          1      11      27      49      83     107     118     123     131   \n",
       "4         47      77     117      79     140      64      55      62      69   \n",
       "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
       "4667     203     186     167      63      30      35      41      52      51   \n",
       "4668      88      90      93     103     115     126     121     126     128   \n",
       "4669      92     102     179     120      73      73      83      83      74   \n",
       "4670      52      65      73      81      87      80     104     133     175   \n",
       "4671      10      23      62      57      67      68      70      72      73   \n",
       "\n",
       "      pixel9  ...  pixel4087  pixel4088  pixel4089  pixel4090  pixel4091  \\\n",
       "0          2  ...        135        108         41          0          0   \n",
       "1         96  ...        129         99         33          0          0   \n",
       "2         20  ...        110         53         13         21         20   \n",
       "3        140  ...        130        149        135        112         99   \n",
       "4         67  ...        138        136        117         68         51   \n",
       "...      ...  ...        ...        ...        ...        ...        ...   \n",
       "4667      50  ...         86         95         77         48         17   \n",
       "4668     129  ...        192        186        177        161        124   \n",
       "4669      67  ...         99         51         10         17         22   \n",
       "4670     203  ...        179        170        171        132         93   \n",
       "4671      75  ...        188        185        178        148         83   \n",
       "\n",
       "      pixel4092  pixel4093  pixel4094  pixel4095  class_id  \n",
       "0             0          0          0          0         1  \n",
       "1             0          0          0          0         1  \n",
       "2            20         20         20         20         0  \n",
       "3            65         32         10          0         2  \n",
       "4            36         17         27         36         1  \n",
       "...         ...        ...        ...        ...       ...  \n",
       "4667          9         15         16         18         1  \n",
       "4668         60         36         48         51         1  \n",
       "4669         24         23         24         23         1  \n",
       "4670         40          0          0          0         0  \n",
       "4671         31         24         26         37         1  \n",
       "\n",
       "[4672 rows x 4097 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "4c17dce9-a425-403e-867c-adf20d9c0ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "\n",
    "columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "print(columns_len)\n",
    "\n",
    "# We have to get the first image in order to initalize array\n",
    "first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "first_img = np.resize(first_img,(64,64))\n",
    "\n",
    "# Get first class to initalize array\n",
    "second = df.loc[0,\"class_id\"]\n",
    "\n",
    "df_X = np.array([first_img],dtype=np.int16) # Image Tensor\n",
    "\n",
    "df_y = np.array([second],dtype=np.int16) # Class Array\n",
    "\n",
    "for row in range(1,len(df)):\n",
    "    \n",
    "    flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "    class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "    matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "\n",
    "    df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "    df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "6b3a5a02-bb63-4490-a80c-ba8cf806d4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672, 64, 64)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "f5514a5b-deae-45ea-bdcd-00324500665f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4672,)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ea1074cf-f930-4feb-a96b-ecf5deee30a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test = df_X[train_indices,:], df_X[test_indices,:] \n",
    "y_train, y_test = df_y[train_indices], df_y[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "4564017d-bc0a-42a0-9a30-fac2198503da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4205, 64, 64) (467, 64, 64) (4205,) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape,X_test.shape,y_train.shape,y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "0a63a626-341a-4d9a-8b75-6431c2d8b16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.from_numpy(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "2dff90c7-6af5-4026-b7d4-2aab34a49d10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "b25b160c-c768-4cf5-b31d-9795f85fc3fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([467, 64, 64])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch = torch.from_numpy(X_test)\n",
    "X_test_torch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "7787a457-e0af-441c-8231-bf81a432da35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
       "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
       "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   1,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
       "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
       "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
       "         ...,\n",
       "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
       "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
       "\n",
       "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
       "         [  0,   2,  26,  ...,  39,  14,   1],\n",
       "         [  0,   0,  21,  ...,  47,  14,   0],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
       "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
       "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
       "         ...,\n",
       "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
       "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
       "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
       "\n",
       "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
       "         [  1,   0,  33,  ...,   0,   0,   0],\n",
       "         [  5,   0,  29,  ...,   0,   0,   4],\n",
       "         ...,\n",
       "         [  5,   0,   0,  ...,   0,   0,   5],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
       "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
       "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
       "         ...,\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
       "         [ 30,  28,  26,  ...,  22,  23,  24]]])"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a54ed148-3e32-488f-aefe-a8785092d1d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "#Testing New Class\n",
    "chest_xray = ChestXRayDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee9d896-bce6-44c0-8d1d-f64f7786dc7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4672"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(chest_xray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "72c4e734-c1e1-4b7d-bc62-37bf62edbe31",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_train = chest_xray[train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d4db8bd-9e46-43dc-8a52-93fb1e9fd335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4205, 64, 64]) (4205,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_train[0].shape, chest_xray_train[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27acb10a-314c-4427-befb-919b7667d8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_xray_test = chest_xray[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7506351d-6cbf-4653-9edc-40b3d43c6e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64]) (467,)\n"
     ]
    }
   ],
   "source": [
    "print(chest_xray_test[0].shape, chest_xray_test[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "444ce630-36cd-4e9f-99ed-48bf5398ee39",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray_train, batch_size = 100, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3b11acb0-437c-4fe3-8dee-9c6795b8da96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1e54e007d90>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f914543b-66dd-42b3-8fb2-e64377acce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataiter = iter(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d663e579-8c0f-4705-a35a-da047fecd376",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m dataiter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28miter\u001b[39m(train_dataloader)\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataiter\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:265\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    205\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;124;03m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;124;03m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;124;03m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:171\u001b[0m, in \u001b[0;36mcollate_numpy_array_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m np_str_obj_array_pattern\u001b[38;5;241m.\u001b[39msearch(elem\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mstr) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(default_collate_err_msg_format\u001b[38;5;241m.\u001b[39mformat(elem\u001b[38;5;241m.\u001b[39mdtype))\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:162\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    160\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    161\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [4205] at entry 0 and [4205, 64, 64] at entry 1"
     ]
    }
   ],
   "source": [
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92075df2-ad1d-43d4-a591-3383e7e0db65",
   "metadata": {},
   "outputs": [],
   "source": [
    "features, labels = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8866d93a-61e1-4a8e-8039-ab21c11c5522",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 65,  72,  86,  ...,  24,  19,  12],\n",
       "         [ 74,  86,  86,  ...,  29,  25,  22],\n",
       "         [ 78,  91,  94,  ...,  30,  28,  24],\n",
       "         ...,\n",
       "         [ 45,  43,  35,  ...,  44,  45,  46],\n",
       "         [ 44,  44,  33,  ...,  45,  46,  46],\n",
       "         [ 44,  43,  31,  ...,  45,  46,  46]],\n",
       "\n",
       "        [[ 48,  61,  59,  ...,  61,  50,  30],\n",
       "         [ 38,  58,  60,  ...,  57,  42,  19],\n",
       "         [ 25,  52,  62,  ...,  51,  40,  15],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   4,   0,   0],\n",
       "         [  0,   0,   0,  ...,   6,   0,   0],\n",
       "         [  0,   0,   0,  ...,   9,   0,   0]],\n",
       "\n",
       "        [[  8,  34,  51,  ..., 192, 195, 211],\n",
       "         [ 12,  48,  48,  ..., 186, 189, 203],\n",
       "         [ 39,  48,  49,  ..., 188, 191, 195],\n",
       "         ...,\n",
       "         [ 32,  30,  29,  ...,  28,  29,  33],\n",
       "         [ 32,  30,  29,  ...,  28,  28,  32],\n",
       "         [ 32,  30,  29,  ...,  28,  29,  31]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[122, 121, 141,  ...,   2,   6,  12],\n",
       "         [146, 129, 118,  ...,  20,  26,  31],\n",
       "         [ 87, 145, 141,  ...,  40,  45,  50],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 18,  34,  48,  ...,  85,  84,  89],\n",
       "         [ 31,  43,  57,  ...,  92, 100,  99],\n",
       "         [ 41,  58,  83,  ..., 109, 112, 104],\n",
       "         ...,\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0],\n",
       "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
       "\n",
       "        [[ 55,  68,  85,  ...,  53,  52,  44],\n",
       "         [ 57,  64,  72,  ...,  56,  47,  44],\n",
       "         [ 59,  69,  79,  ...,  61,  51,  49],\n",
       "         ...,\n",
       "         [ 21,  14,  14,  ...,  14,  20,  26],\n",
       "         [ 19,  14,  16,  ...,  17,  16,  25],\n",
       "         [ 19,  13,  18,  ...,  22,  13,  26]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6fc5e629-1563-4141-82fa-3f5d44f7e4b1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[  0,  17,  60,  ..., 121, 120,  93],\n",
      "         [  0,  15,  57,  ..., 117, 111,  91],\n",
      "         [  0,  13,  54,  ..., 118, 108,  87],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 19,  19,  15,  ...,  48,  45,  36],\n",
      "         [ 20,  19,   3,  ...,  43,  45,  33],\n",
      "         [ 16,  10,  16,  ...,  45,  37,  90],\n",
      "         ...,\n",
      "         [ 30,  29,  29,  ...,  28,  29,  30],\n",
      "         [ 30,  30,  29,  ...,  28,  29,  30],\n",
      "         [ 32,  29,  29,  ...,  29,  28,  32]],\n",
      "\n",
      "        [[ 40,  44,  44,  ...,  16,  23,  14],\n",
      "         [ 39,  43,  49,  ...,  14,  20,  14],\n",
      "         [ 41,  46,  55,  ...,  11,  16,  16],\n",
      "         ...,\n",
      "         [ 26,  25,  18,  ...,  27,  27,  28],\n",
      "         [ 26,  25,  17,  ...,  27,  27,  28],\n",
      "         [ 25,  24,  16,  ...,  27,  28,  28]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[117, 115, 122,  ..., 102, 114, 132],\n",
      "         [130, 123, 127,  ..., 110, 120, 128],\n",
      "         [121, 140, 134,  ..., 110, 120, 127],\n",
      "         ...,\n",
      "         [  3,  30,  58,  ...,  10,   0,   0],\n",
      "         [  3,  30,  61,  ...,  12,   0,   0],\n",
      "         [  5,  31,  62,  ...,  15,   0,   1]],\n",
      "\n",
      "        [[116,  88,  80,  ..., 147, 130, 120],\n",
      "         [ 97, 103, 115,  ..., 153, 151, 138],\n",
      "         [ 87,  92, 110,  ..., 145, 169, 153],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 34,  48,  56,  ...,  64,  47, 155],\n",
      "         [ 48,  59,  64,  ...,  79,  75, 172],\n",
      "         [ 48,  63,  71,  ...,  37,  41, 144],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ..., 117,  33,   1],\n",
      "         [  0,   0,   0,  ...,  99,  36,   2],\n",
      "         [  0,   0,   0,  ...,  89,  37,   3]]]) tensor([0, 1, 1, 2, 1, 2, 1, 0, 1, 1, 1, 1, 2, 0, 1, 2, 2, 0, 0, 2, 0, 2, 1, 1,\n",
      "        1, 1, 0, 0, 1, 1, 1, 2, 1, 2, 2, 1, 0, 1, 0, 1, 2, 0, 1, 2, 1, 1, 1, 0,\n",
      "        1, 0, 1, 2, 0, 1, 1, 2, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 0, 1, 2, 2, 0,\n",
      "        0, 0, 2, 2, 2, 0, 0, 1, 1, 1, 0, 0, 1, 0, 2, 0, 1, 1, 1, 2, 1, 1, 1, 2,\n",
      "        1, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "8ef92e41-37c9-48cf-8b36-cefefac420ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n"
     ]
    }
   ],
   "source": [
    "# Test if train = True and Train = False works\n",
    "chest_xray = ChestXRayDataset(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5829ecd4-a130-48bb-ae3e-035b03beac0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205, 64, 64])"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ffb4bc3b-54fe-4f16-a599-c46d8fe3d6f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4205])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chest_xray.y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "34639f73-ab63-4764-ab33-09445ee2f853",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(dataset = chest_xray, batch_size = 100, shuffle = True)\n",
    "dataiter = iter(train_dataloader)\n",
    "data = next(dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0ccd7686-f592-4d0d-a3e7-901bf8d981e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[20, 32, 41,  ..., 83, 80, 76],\n",
       "        [32, 47, 58,  ..., 82, 75, 68],\n",
       "        [39, 57, 72,  ..., 80, 71, 59],\n",
       "        ...,\n",
       "        [ 0,  0,  0,  ..., 17,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 23,  0,  1],\n",
       "        [ 0,  0,  0,  ..., 27,  0,  1]])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "a93895ac-2cea-4424-a6af-db9453992995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 31,  40,  53,  ...,  18,  23,  23],\n",
      "         [ 35,  46,  60,  ...,  14,  16,  23],\n",
      "         [ 37,  54,  69,  ...,  32,  14,  11],\n",
      "         ...,\n",
      "         [ 19,  13,  18,  ...,  22,  23,  24],\n",
      "         [ 19,  13,  19,  ...,  23,  24,  25],\n",
      "         [ 18,  13,  19,  ...,  24,  27,  25]],\n",
      "\n",
      "        [[ 77,  83,  92,  ...,   0,   0,   0],\n",
      "         [128, 101,  91,  ...,  10,  18,  27],\n",
      "         [137, 150, 128,  ...,  45,  51,  57],\n",
      "         ...,\n",
      "         [  1,  25,  64,  ...,   2,   0,   0],\n",
      "         [  2,  26,  74,  ...,   3,   0,   0],\n",
      "         [  2,  28,  82,  ...,   5,   0,   0]],\n",
      "\n",
      "        [[  0,   0,   4,  ...,   0,   0,   0],\n",
      "         [  0,   0,   8,  ...,   0,   2,   0],\n",
      "         [  0,   0,  14,  ...,   0,   5,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 55,  76,  93,  ...,  78,  73,  54],\n",
      "         [ 52,  74,  89,  ...,  79,  69,  49],\n",
      "         [ 48,  72,  88,  ...,  85,  67,  45],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   9,   0,   1],\n",
      "         [  0,   1,   0,  ...,  11,   0,   1],\n",
      "         [  0,   1,   0,  ...,  12,   0,   1]],\n",
      "\n",
      "        [[ 18,  36,  61,  ...,  59,  44,  39],\n",
      "         [ 56,  62,  42,  ...,  41,  39,  44],\n",
      "         [ 41,  39,  45,  ...,  41,  38,  51],\n",
      "         ...,\n",
      "         [ 33,  31,  32,  ...,  13,  21,  21],\n",
      "         [ 33,  30,  31,  ...,  13,  21,  22],\n",
      "         [ 34,  30,  30,  ...,  13,  20,  22]],\n",
      "\n",
      "        [[ 88,  13,  26,  ..., 192, 204, 225],\n",
      "         [ 23,  17,  39,  ..., 193, 208, 224],\n",
      "         [ 17,  24,  41,  ..., 194, 208, 221],\n",
      "         ...,\n",
      "         [ 28,  25,  23,  ...,  25,  25,  30],\n",
      "         [ 27,  25,  23,  ...,  25,  25,  29],\n",
      "         [ 26,  25,  22,  ...,  25,  26,  27]]]) tensor([1, 0, 2, 2, 2, 0, 1, 1, 0, 1, 0, 0, 0, 2, 1, 1, 2, 0, 0, 1, 1, 2, 2, 0,\n",
      "        1, 1, 1, 1, 1, 1, 1, 2, 1, 0, 2, 2, 1, 0, 0, 1, 1, 2, 1, 1, 0, 1, 2, 1,\n",
      "        2, 1, 0, 1, 2, 1, 1, 1, 0, 2, 1, 0, 0, 1, 2, 1, 1, 1, 1, 2, 1, 2, 2, 1,\n",
      "        0, 0, 2, 1, 0, 2, 1, 2, 2, 0, 2, 1, 2, 0, 0, 2, 0, 2, 2, 0, 1, 0, 1, 2,\n",
      "        1, 0, 1, 1])\n",
      "torch.Size([100, 64, 64]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape, y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "8f3905e9-48d6-4a17-94c5-025d839a761d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([467, 64, 64])\n",
      "torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "chest_xray_validation = ChestXRayDataset(False)\n",
    "print(chest_xray_validation.X.shape)\n",
    "print(chest_xray_validation.y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "87771576-dccc-46d7-a20b-d6f6a9c9ea28",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_dataloader = DataLoader(dataset = chest_xray_validation, batch_size = 500)\n",
    "valid_dataiter = iter(validation_dataloader)\n",
    "valid_data = next(valid_dataiter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f53f02b4-3a91-439d-81b5-152fcdf000e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 55,  63,  69,  ..., 153, 165, 138],\n",
      "         [ 62,  72,  81,  ..., 156, 135, 117],\n",
      "         [ 60,  80,  89,  ..., 139, 132, 116],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   1,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 38,  51,  56,  ...,  66,  46,  30],\n",
      "         [ 37,  50,  55,  ...,  65,  46,  30],\n",
      "         [ 35,  51,  58,  ...,  63,  46,  26],\n",
      "         ...,\n",
      "         [ 15,  12,  47,  ...,  22,  25,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  24],\n",
      "         [ 15,  12,  46,  ...,  23,  24,  25]],\n",
      "\n",
      "        [[  0,   4,  31,  ...,  31,  19,   6],\n",
      "         [  0,   2,  26,  ...,  39,  14,   1],\n",
      "         [  0,   0,  21,  ...,  47,  14,   0],\n",
      "         ...,\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 62,  61,  62,  ..., 131, 157, 179],\n",
      "         [ 57,  57,  57,  ..., 125, 165, 178],\n",
      "         [ 50,  54,  48,  ..., 119, 163, 179],\n",
      "         ...,\n",
      "         [ 84, 125, 147,  ..., 162, 165, 143],\n",
      "         [ 90, 125, 145,  ..., 156, 162, 145],\n",
      "         [ 94, 126, 144,  ..., 150, 160, 147]],\n",
      "\n",
      "        [[  2,   0,  41,  ...,   0,   0,   0],\n",
      "         [  1,   0,  33,  ...,   0,   0,   0],\n",
      "         [  5,   0,  29,  ...,   0,   0,   4],\n",
      "         ...,\n",
      "         [  5,   0,   0,  ...,   0,   0,   5],\n",
      "         [  0,   0,   0,  ...,   0,   0,   0],\n",
      "         [  0,   0,   1,  ...,   0,   0,   0]],\n",
      "\n",
      "        [[ 71,  62,  63,  ...,  42,  42,  47],\n",
      "         [ 76,  61,  49,  ...,  46,  43,  49],\n",
      "         [ 62,  56,  67,  ...,  57,  49,  50],\n",
      "         ...,\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 29,  28,  26,  ...,  22,  23,  23],\n",
      "         [ 30,  28,  26,  ...,  22,  23,  24]]]) tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0,\n",
      "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0,\n",
      "        1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1,\n",
      "        0, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 1, 0, 0, 2, 2, 2, 1, 1, 1, 1,\n",
      "        1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 2, 2, 1, 1, 2, 2, 1, 1, 1, 2, 1, 1, 0, 2,\n",
      "        1, 1, 2, 0, 2, 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 0, 2, 0, 2, 2, 2, 2, 1,\n",
      "        1, 2, 1, 2, 2, 2, 2, 1, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        2, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 2, 2, 0, 1, 0, 2, 1, 1, 0, 0, 0, 1,\n",
      "        2, 2, 2, 1, 2, 1, 2, 1, 0, 2, 0, 0, 2, 2, 0, 0, 2, 1, 1, 1, 1, 2, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 1, 1, 1, 2, 0, 2, 0, 2, 0, 1, 2, 2, 2, 1,\n",
      "        0, 1, 1, 1, 1, 0, 2, 2, 1, 0, 0, 2, 0, 0, 1, 1, 2, 2, 2, 1, 0, 1, 0, 0,\n",
      "        1, 0, 2, 0, 2, 0, 0, 0, 1, 2, 1, 2, 1, 0, 1, 1, 0, 2, 1, 0, 1, 2, 2, 2,\n",
      "        0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 2, 1, 0, 0, 1, 1, 1, 2, 2, 1, 1,\n",
      "        0, 0, 1, 1, 1, 2, 0, 0, 1, 0, 2, 2, 1, 1, 2, 1, 1, 2, 1, 2, 1, 1, 0, 1,\n",
      "        0, 1, 2, 2, 1, 2, 1, 0, 2, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1,\n",
      "        1, 2, 1, 1, 0, 0, 2, 1, 1, 1, 1, 2, 0, 1, 1, 0, 0, 1, 0, 2, 1, 1, 2, 1,\n",
      "        1, 2, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1,\n",
      "        1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
      "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])\n",
      "torch.Size([467, 64, 64]) torch.Size([467])\n"
     ]
    }
   ],
   "source": [
    "for x,y in validation_dataloader:\n",
    "    print(x,y)\n",
    "    print(x.shape,y.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fc203ee7-24b1-42f0-98d2-8d516c667f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNN please work time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9331f960-9c7a-4a2c-94d1-44f9cc71ca2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Library\n",
    "import torch\n",
    "# PyTorch Neural Network\n",
    "import torch.nn as nn\n",
    "# Allows us to transform data\n",
    "import torchvision.transforms as transforms\n",
    "# Allows us to download the dataset\n",
    "import torchvision.datasets as dsets\n",
    "# Used to graph data and loss curves\n",
    "import matplotlib.pylab as plt\n",
    "# Allows us to use arrays to manipulate and store data\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19d2198d-716b-4ef1-8f2f-d1c7fd7ce1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function for plotting the channels\n",
    "\n",
    "def plot_channels(W):\n",
    "    n_out = W.shape[0]\n",
    "    n_in = W.shape[1]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(n_out, n_in)\n",
    "    fig.subplots_adjust(hspace=0.1)\n",
    "    out_index = 0\n",
    "    in_index = 0\n",
    "    \n",
    "    #plot outputs as rows inputs as columns \n",
    "    for ax in axes.flat:\n",
    "        if in_index > n_in-1:\n",
    "            out_index = out_index + 1\n",
    "            in_index = 0\n",
    "        ax.imshow(W[out_index, in_index, :, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "        ax.set_yticklabels([])\n",
    "        ax.set_xticklabels([])\n",
    "        in_index = in_index + 1\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the parameters\n",
    "\n",
    "def plot_parameters(W, number_rows=1, name=\"\", i=0):\n",
    "    W = W.data[:, i, :, :]\n",
    "    n_filters = W.shape[0]\n",
    "    w_min = W.min().item()\n",
    "    w_max = W.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_filters // number_rows)\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_filters:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"kernel:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(W[i, :], vmin=w_min, vmax=w_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.suptitle(name, fontsize=10)    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Define the function for plotting the activations\n",
    "\n",
    "def plot_activations(A, number_rows=1, name=\"\", i=0):\n",
    "    A = A[0, :, :, :].detach().numpy()\n",
    "    n_activations = A.shape[0]\n",
    "    A_min = A.min().item()\n",
    "    A_max = A.max().item()\n",
    "    fig, axes = plt.subplots(number_rows, n_activations // number_rows)\n",
    "    fig.subplots_adjust(hspace = 0.9)    \n",
    "\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        if i < n_activations:\n",
    "            # Set the label for the sub-plot.\n",
    "            ax.set_xlabel(\"activation:{0}\".format(i + 1))\n",
    "\n",
    "            # Plot the image.\n",
    "            ax.imshow(A[i, :], vmin=A_min, vmax=A_max, cmap='seismic')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def show_data(data_sample):\n",
    "    plt.imshow(data_sample[0].numpy().reshape(IMAGE_SIZE, IMAGE_SIZE), cmap='gray')\n",
    "    plt.title('y = '+ str(data_sample[1].item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1cd973b1-a617-4db0-a1f3-c355d5ab14cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "\n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.float32) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.float32) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "            # df_X = df_X.astype(np.float32)\n",
    "            # df_y = df_y.astype(np.float32)\n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            # df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            # df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f96e4f2a-b3c8-4e10-a5d7-860c75440b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    \n",
    "    # Contructor\n",
    "    def __init__(self, out_1=16, out_2=32):\n",
    "        super(CNN, self).__init__()\n",
    "        # The reason we start with 1 channel is because we have a single black and white image\n",
    "        # Channel Width after this layer is 16\n",
    "        self.cnn1 = nn.Conv2d(in_channels=1, out_channels=out_1, kernel_size=5, padding=2)\n",
    "        # Channel Wifth after this layer is 8\n",
    "        self.maxpool1=nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        # Channel Width after this layer is 8\n",
    "        self.cnn2 = nn.Conv2d(in_channels=out_1, out_channels=out_2, kernel_size=5, stride=1, padding=2)\n",
    "        # Channel Width after this layer is 4\n",
    "        self.maxpool2=nn.MaxPool2d(kernel_size=2)\n",
    "        # In total we have out_2 (32) channels which are each 4 * 4 in size based on the width calculation above. Channels are squares.\n",
    "        # The output is a value for each class\n",
    "        self.fc1 = nn.Linear(out_2 * 4 * 4, 10)\n",
    "    \n",
    "    # Prediction\n",
    "    def forward(self, x):\n",
    "        # Puts the X value through each cnn, relu, and pooling layer and it is flattened for input into the fully connected layer\n",
    "        x = self.cnn1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.cnn2(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "    # Outputs result of each stage of the CNN, relu, and pooling layers\n",
    "    def activations(self, x):\n",
    "        # Outputs activation this is not necessary\n",
    "        z1 = self.cnn1(x)\n",
    "        a1 = torch.relu(z1)\n",
    "        out = self.maxpool1(a1)\n",
    "        \n",
    "        z2 = self.cnn2(out)\n",
    "        a2 = torch.relu(z2)\n",
    "        out1 = self.maxpool2(a2)\n",
    "        out = out.view(out.size(0),-1)\n",
    "        return z1, a1, z2, a2, out1,out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3162fdcb-166e-433a-9f41-d485b36f3e8c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 32 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = CNN(out_1=16, out_2=32)\n",
    "plot_parameters(model.state_dict()['cnn1.weight'], number_rows=4, name=\"1st layer kernels before training \")\n",
    "plot_parameters(model.state_dict()['cnn2.weight'], number_rows=4, name='2nd layer kernels before training' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24b446dc-e5f0-4226-96c4-121c1440d33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n",
      "4097\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChestXRayDataset(True)\n",
    "validation_dataset = ChestXRayDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8e55608-98ae-470b-82f9-4f12c3f857b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.X = train_dataset.X.float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353a691-135f-4d43-85e8-a911b7147f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dat    = torch.utils.data.TensorDataset(torch.tensor(test_data_x).to(device), torch.tensor(test_data_y).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c70975-3591-4b4c-a208-9bca4d4a25c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset.X.set_default_dtype(torch.float32)\n",
    "# train_dataset.X.float()\n",
    "train_dataset.y.int()\n",
    "# validation_dataset.X.float()\n",
    "validation_dataset.X.set_default_dtype(torch.float32)\n",
    "validation_dataset.y.int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "bf7d0270-1c1a-4646-a5c9-03a225ad7eca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 55.,  63.,  69.,  ..., 153., 165., 138.],\n",
       "         [ 62.,  72.,  81.,  ..., 156., 135., 117.],\n",
       "         [ 60.,  80.,  89.,  ..., 139., 132., 116.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.,  ...,   0.,   1.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   1.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 38.,  51.,  56.,  ...,  66.,  46.,  30.],\n",
       "         [ 37.,  50.,  55.,  ...,  65.,  46.,  30.],\n",
       "         [ 35.,  51.,  58.,  ...,  63.,  46.,  26.],\n",
       "         ...,\n",
       "         [ 15.,  12.,  47.,  ...,  22.,  25.,  24.],\n",
       "         [ 15.,  12.,  46.,  ...,  23.,  24.,  24.],\n",
       "         [ 15.,  12.,  46.,  ...,  23.,  24.,  25.]],\n",
       "\n",
       "        [[  0.,   4.,  31.,  ...,  31.,  19.,   6.],\n",
       "         [  0.,   2.,  26.,  ...,  39.,  14.,   1.],\n",
       "         [  0.,   0.,  21.,  ...,  47.,  14.,   0.],\n",
       "         ...,\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 62.,  61.,  62.,  ..., 131., 157., 179.],\n",
       "         [ 57.,  57.,  57.,  ..., 125., 165., 178.],\n",
       "         [ 50.,  54.,  48.,  ..., 119., 163., 179.],\n",
       "         ...,\n",
       "         [ 84., 125., 147.,  ..., 162., 165., 143.],\n",
       "         [ 90., 125., 145.,  ..., 156., 162., 145.],\n",
       "         [ 94., 126., 144.,  ..., 150., 160., 147.]],\n",
       "\n",
       "        [[  2.,   0.,  41.,  ...,   0.,   0.,   0.],\n",
       "         [  1.,   0.,  33.,  ...,   0.,   0.,   0.],\n",
       "         [  5.,   0.,  29.,  ...,   0.,   0.,   4.],\n",
       "         ...,\n",
       "         [  5.,   0.,   0.,  ...,   0.,   0.,   5.],\n",
       "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
       "         [  0.,   0.,   1.,  ...,   0.,   0.,   0.]],\n",
       "\n",
       "        [[ 71.,  62.,  63.,  ...,  42.,  42.,  47.],\n",
       "         [ 76.,  61.,  49.,  ...,  46.,  43.,  49.],\n",
       "         [ 62.,  56.,  67.,  ...,  57.,  49.,  50.],\n",
       "         ...,\n",
       "         [ 29.,  28.,  26.,  ...,  22.,  23.,  23.],\n",
       "         [ 29.,  28.,  26.,  ...,  22.,  23.,  23.],\n",
       "         [ 30.,  28.,  26.,  ...,  22.,  23.,  24.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_dataset.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2ffe137c-755f-4b2d-9d19-9848c75d5c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create a criterion which will measure loss\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "learning_rate = 0.1\n",
    "# Create an optimizer that updates model parameters using the learning rate and gradient\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "# Create a Data Loader for the training data with a batch size of 100 \n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=1)\n",
    "# Create a Data Loader for the validation data with a batch size of 5000 \n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dd09b9ee-f883-4b1d-a6e6-edcd9d432fbe",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x256 and 512x10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 52\u001b[0m\n\u001b[0;32m     49\u001b[0m         accuracy \u001b[38;5;241m=\u001b[39m correct \u001b[38;5;241m/\u001b[39m N_test\n\u001b[0;32m     50\u001b[0m         accuracy_list\u001b[38;5;241m.\u001b[39mappend(accuracy)\n\u001b[1;32m---> 52\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[18], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(n_epochs)\u001b[0m\n\u001b[0;32m     20\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Makes a prediction based on X value\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# print(x)\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# Measures the loss between prediction and acutal Y value\u001b[39;00m\n\u001b[0;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(z, y)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagerecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagerecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[4], line 30\u001b[0m, in \u001b[0;36mCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool2(x)\n\u001b[0;32m     29\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagerecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagerecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\imagerecognition\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x256 and 512x10)"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Number of times we want to train on the taining dataset\n",
    "n_epochs=3\n",
    "# List to keep track of cost and accuracy\n",
    "cost_list=[]\n",
    "accuracy_list=[]\n",
    "# Size of the validation dataset\n",
    "N_test=len(validation_dataset)\n",
    "\n",
    "# Model Training Function\n",
    "def train_model(n_epochs):\n",
    "    # Loops for each epoch\n",
    "    for epoch in range(n_epochs):\n",
    "        # Keeps track of cost for each epoch\n",
    "        COST=0\n",
    "        # For each batch in train loader\n",
    "        for x, y in train_loader:\n",
    "            # Resets the calculated gradient value, this must be done each time as it accumulates if we do not reset\n",
    "            optimizer.zero_grad()\n",
    "            # Makes a prediction based on X value\n",
    "\n",
    "            # print(x)\n",
    "            # break\n",
    "            z = model(x)\n",
    "            # Measures the loss between prediction and acutal Y value\n",
    "            loss = criterion(z, y)\n",
    "            # Calculates the gradient value with respect to each weight and bias\n",
    "            loss.backward()\n",
    "            # Updates the weight and bias according to calculated gradient value\n",
    "            optimizer.step()\n",
    "            # Cumulates loss \n",
    "            COST+=loss.data\n",
    "        \n",
    "        # Saves cost of training data of epoch\n",
    "        cost_list.append(COST)\n",
    "        # Keeps track of correct predictions\n",
    "        correct=0\n",
    "        # Perform a prediction on the validation  data  \n",
    "        for x_test, y_test in validation_loader:\n",
    "            # Makes a prediction\n",
    "            z = model(x_test)\n",
    "            # The class with the max value is the one we are predicting\n",
    "            _, yhat = torch.max(z.data, 1)\n",
    "            # Checks if the prediction matches the actual value\n",
    "            correct += (yhat == y_test).sum().item()\n",
    "        \n",
    "        # Calcualtes accuracy and saves it\n",
    "        accuracy = correct / N_test\n",
    "        accuracy_list.append(accuracy)\n",
    "     \n",
    "train_model(n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1f158610-53e1-4b3c-b3ea-8e65cd20cbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47f52abd-4bd4-4296-a30d-5adab9a17c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.array([1,2,3],dtype=np.int16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "369ced57-ebfc-492b-ac6b-869bff5d879f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3], dtype=int16)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb211127-2ab7-4dea-8f8a-e062bda41662",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.int16"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2859b85e-06bc-4406-a349-bcb6709d579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = a.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f7a3d53-c1b6-4372-8461-49bd71bbad3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2., 3.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83398d5d-85a1-4616-b659-0f976933b8df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.float32"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(a[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f780ed74-c63e-4e5a-99dc-a8c8f51c9105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
