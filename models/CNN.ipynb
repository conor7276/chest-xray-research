{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91fe79e-0df0-482f-bbbf-e066abbd7128",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "688f4ca2-5fa2-4bbe-ba82-c68ab7d20270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameters\n",
    "num_epochs = 4\n",
    "batch_size = 4\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36e3c367-b65d-4c57-b23d-e855d10a4841",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3534d94e-0c64-48c2-bbb3-97c40b94b7bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tutorial data loading\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "class ChestXRayDataset(Dataset):\n",
    "\n",
    "    def __init__(self,train):\n",
    "        # df = pd.read_csv('./data/chest_xray_train.csv')\n",
    "        \n",
    "        #Load Static testing and training data indices\n",
    "        \n",
    "        with open('static_data/train.pickle','rb') as handle:\n",
    "            train_indices = pickle.load(handle)\n",
    "        \n",
    "        with open('static_data/test.pickle','rb') as handle:\n",
    "            test_indices = pickle.load(handle)\n",
    "\n",
    "        df = pd.read_csv('data/chest_xray_train.csv')\n",
    "\n",
    "        df = df.drop(columns = [\"file_name\"]) # unnecessary column\n",
    "\n",
    "        columns_len = len(df.columns) # Get number of columns in dataframe\n",
    "        print(columns_len)\n",
    "        \n",
    "        # We have to get the first image in order to initalize array\n",
    "        first_img = df.iloc[0,0:columns_len-1].to_numpy()\n",
    "        first_img = np.resize(first_img,(64,64))\n",
    "        \n",
    "        # Get first class to initalize array\n",
    "        second = df.loc[0,\"class_id\"]\n",
    "        \n",
    "        df_X = np.array([first_img],dtype=np.float32) # Image Tensor\n",
    "        \n",
    "        df_y = np.array([second],dtype=np.float32) # Class Array\n",
    "        \n",
    "        for row in range(1,len(df)):\n",
    "            \n",
    "            flattened_img = df.iloc[row,0:columns_len-1].to_numpy() # skip class_id\n",
    "            class_id = df.loc[row,\"class_id\"] # get class_id\n",
    "            matrix_img = np.resize(flattened_img,(64,64)) # turn flattened image back into 2d image\n",
    "\n",
    "            # df_X = df_X.astype(np.float32)\n",
    "            # df_y = df_y.astype(np.float32)\n",
    "        \n",
    "            df_X = np.append(df_X, [matrix_img],axis = 0) # append image\n",
    "            df_y = np.append(df_y, class_id) # append class\n",
    "\n",
    "        if train == True:\n",
    "            # df_X, df_y = df_X[train_indices], df_y[train_indices] #X_train, y_train\n",
    "            df_X, df_y = df_X[train_indices], df_X[test_indices]\n",
    "            # df_y = torch.from_numpy(df_y)\n",
    "        else:\n",
    "            # df_X, df_y = df_X[test_indices], df_y[test_indices] #y_train, y_test\n",
    "            df_X, df_y = df_y[train_indices], df_y[test_indices] #y_train, y_test\n",
    "            # df_y = torch.from_numpy(df_y)\n",
    "            \n",
    "        X_torch = torch.from_numpy(df_X)\n",
    "        # y_torch = torch.from_numpy(df_y)\n",
    "\n",
    "        self.X = X_torch\n",
    "        # self.y = y_torch\n",
    "        self.y = df_y\n",
    "\n",
    "    def __getitem__(self,index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fef452b-776d-405d-943a-fb06d4e8da2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n",
      "4097\n"
     ]
    }
   ],
   "source": [
    "train_dataset = ChestXRayDataset(True)\n",
    "validation_dataset = ChestXRayDataset(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a57b138c-7c79-433a-8819-17620ad95341",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size = batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a389630-0d97-48c7-a6f3-af5fbe2577e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = (0,1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a3d489e6-eddc-4029-8f8e-fd66176aa150",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def forward(self,x):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1ccee946-58d8-437c-b96d-b528e6433604",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConvNet' object has no attribute '_modules'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mConvNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m      3\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mSGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m learning_rate)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001b[0m, in \u001b[0;36mModule.to\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1157\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m   1158\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[1;32m-> 1160\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:809\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, recurse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m--> 809\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchildren\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m    810\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2284\u001b[0m, in \u001b[0;36mModule.children\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2278\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchildren\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Iterator[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModule\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m   2279\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules.\u001b[39;00m\n\u001b[0;32m   2280\u001b[0m \n\u001b[0;32m   2281\u001b[0m \u001b[38;5;124;03m    Yields:\u001b[39;00m\n\u001b[0;32m   2282\u001b[0m \u001b[38;5;124;03m        Module: a child module\u001b[39;00m\n\u001b[0;32m   2283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 2284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnamed_children\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m   2285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmodule\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:2303\u001b[0m, in \u001b[0;36mModule.named_children\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2288\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Returns an iterator over immediate children modules, yielding both\u001b[39;00m\n\u001b[0;32m   2289\u001b[0m \u001b[38;5;124;03mthe name of the module as well as the module itself.\u001b[39;00m\n\u001b[0;32m   2290\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   2300\u001b[0m \n\u001b[0;32m   2301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2302\u001b[0m memo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m-> 2303\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_modules\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2304\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m module \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m memo:\n\u001b[0;32m   2305\u001b[0m         memo\u001b[38;5;241m.\u001b[39madd(module)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1693\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1694\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConvNet' object has no attribute '_modules'"
     ]
    }
   ],
   "source": [
    "model = ConvNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = learning_rate)\n",
    "n_total_steps = len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f9c6b65d-ed50-4746-8920-f7d27643106e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m(images)\n\u001b[0;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb1b6de1-e2f9-42ef-95a4-36bfbdcd87eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [2000/12500], Loss: 2.3133\n",
      "Epoch [1/5], Step [4000/12500], Loss: 2.3026\n",
      "Epoch [1/5], Step [6000/12500], Loss: 2.2756\n",
      "Epoch [1/5], Step [8000/12500], Loss: 2.2303\n",
      "Epoch [1/5], Step [10000/12500], Loss: 2.3690\n",
      "Epoch [1/5], Step [12000/12500], Loss: 2.1108\n",
      "Epoch [2/5], Step [2000/12500], Loss: 2.1159\n",
      "Epoch [2/5], Step [4000/12500], Loss: 2.0568\n",
      "Epoch [2/5], Step [6000/12500], Loss: 1.9809\n",
      "Epoch [2/5], Step [8000/12500], Loss: 1.8931\n",
      "Epoch [2/5], Step [10000/12500], Loss: 1.5951\n",
      "Epoch [2/5], Step [12000/12500], Loss: 1.8156\n",
      "Epoch [3/5], Step [2000/12500], Loss: 1.6594\n",
      "Epoch [3/5], Step [4000/12500], Loss: 1.5642\n",
      "Epoch [3/5], Step [6000/12500], Loss: 2.0569\n",
      "Epoch [3/5], Step [8000/12500], Loss: 1.6733\n",
      "Epoch [3/5], Step [10000/12500], Loss: 1.2532\n",
      "Epoch [3/5], Step [12000/12500], Loss: 2.0487\n",
      "Epoch [4/5], Step [2000/12500], Loss: 2.1321\n",
      "Epoch [4/5], Step [4000/12500], Loss: 2.3567\n",
      "Epoch [4/5], Step [6000/12500], Loss: 1.8798\n",
      "Epoch [4/5], Step [8000/12500], Loss: 1.1026\n",
      "Epoch [4/5], Step [10000/12500], Loss: 1.1106\n",
      "Epoch [4/5], Step [12000/12500], Loss: 1.1782\n",
      "Epoch [5/5], Step [2000/12500], Loss: 0.9972\n",
      "Epoch [5/5], Step [4000/12500], Loss: 1.5118\n",
      "Epoch [5/5], Step [6000/12500], Loss: 1.4922\n",
      "Epoch [5/5], Step [8000/12500], Loss: 0.5880\n",
      "Epoch [5/5], Step [10000/12500], Loss: 2.1864\n",
      "Epoch [5/5], Step [12000/12500], Loss: 1.8453\n",
      "Finished Training\n",
      "Accuracy of the network: 49.9 %\n",
      "Accuracy of plane: 53.3 %\n",
      "Accuracy of car: 69.1 %\n",
      "Accuracy of bird: 22.9 %\n",
      "Accuracy of cat: 30.5 %\n",
      "Accuracy of deer: 34.4 %\n",
      "Accuracy of dog: 42.2 %\n",
      "Accuracy of frog: 61.2 %\n",
      "Accuracy of horse: 70.3 %\n",
      "Accuracy of ship: 53.5 %\n",
      "Accuracy of truck: 61.6 %\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=transform)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                          shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "                                         shuffle=False)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat',\n",
    "           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "def imshow(img):\n",
    "    img = img / 2 + 0.5  # unnormalize\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 5 * 5)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c666b13-30b4-4436-a85d-aa3f4f2a61e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4097\n",
      "4097\n"
     ]
    }
   ],
   "source": [
    "#################\n",
    "train_dataset = ChestXRayDataset(True)\n",
    "validation_dataset = ChestXRayDataset(False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset=validation_dataset, batch_size = batch_size)\n",
    "\n",
    "classes = (0,1,2)\n",
    "train_dataset.X = train_dataset.X.float()\n",
    "#################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4a34f-afe6-42b2-b2c8-b160e8b34ab3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f9352fa5-b7d9-4de0-9c4a-fe1d3fce154b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (2x16384 and 8192x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 91\u001b[0m\n\u001b[0;32m     89\u001b[0m images \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(images,(\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m64\u001b[39m,\u001b[38;5;241m64\u001b[39m))\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     92\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     94\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[29], line 70\u001b[0m, in \u001b[0;36mConvNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     68\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))  \u001b[38;5;66;03m# -> n, 16, 5, 5\u001b[39;00m\n\u001b[0;32m     69\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m32\u001b[39m)            \u001b[38;5;66;03m# -> n, 400\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)               \u001b[38;5;66;03m# -> n, 120\u001b[39;00m\n\u001b[0;32m     71\u001b[0m x \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))               \u001b[38;5;66;03m# -> n, 84\u001b[39;00m\n\u001b[0;32m     72\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)                       \u001b[38;5;66;03m# -> n, 10\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\ImageRecognition\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (2x16384 and 8192x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyper-parameters \n",
    "num_epochs = 5\n",
    "batch_size = 4\n",
    "learning_rate = 0.001\n",
    "\n",
    "# dataset has PILImage images of range [0, 1]. \n",
    "# We transform them to Tensors of normalized range [-1, 1]\n",
    "# transform = transforms.Compose(\n",
    "#     [transforms.ToTensor(),\n",
    "#      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "# # CIFAR10: 60000 32x32 color images in 10 classes, with 6000 images per class\n",
    "# train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "#                                         download=True, transform=transform)\n",
    "\n",
    "# test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
    "#                                        download=True, transform=transform)\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "#                                           shuffle=True)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,\n",
    "#                                          shuffle=False)\n",
    "\n",
    "# classes = ('plane', 'car', 'bird', 'cat',\n",
    "#            'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
    "\n",
    "# def imshow(img):\n",
    "#     img = img / 2 + 0.5  # unnormalize\n",
    "#     npimg = img.numpy()\n",
    "#     plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "# imshow(torchvision.utils.make_grid(images))\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, 3, 1, 1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, 1, 1)\n",
    "        self.fc1 = nn.Linear(32 * 16 * 16, 128)\n",
    "        self.fc2 = nn.Linear(128, 84)\n",
    "        self.fc3 = nn.Linear(84, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # -> n, 3, 32, 32\n",
    "        x = self.pool(F.relu(self.conv1(x)))  # -> n, 6, 14, 14\n",
    "        x = self.pool(F.relu(self.conv2(x)))  # -> n, 16, 5, 5\n",
    "        x = x.view(-1, 16 * 32 * 32)            # -> n, 400\n",
    "        x = F.relu(self.fc1(x))               # -> n, 120\n",
    "        x = F.relu(self.fc2(x))               # -> n, 84\n",
    "        x = self.fc3(x)                       # -> n, 10\n",
    "        return x\n",
    "\n",
    "\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "n_total_steps = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # origin shape: [4, 3, 32, 32] = 4, 3, 1024\n",
    "        # input_layer: 3 input channels, 6 output channels, 5 kernel size\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = torch.reshape(images,(4,1,64,64))\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 2000 == 0:\n",
    "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{n_total_steps}], Loss: {loss.item():.4f}')\n",
    "\n",
    "print('Finished Training')\n",
    "PATH = './cnn.pth'\n",
    "torch.save(model.state_dict(), PATH)\n",
    "\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    n_class_correct = [0 for i in range(10)]\n",
    "    n_class_samples = [0 for i in range(10)]\n",
    "    for images, labels in test_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        # max returns (value ,index)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        n_samples += labels.size(0)\n",
    "        n_correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            label = labels[i]\n",
    "            pred = predicted[i]\n",
    "            if (label == pred):\n",
    "                n_class_correct[label] += 1\n",
    "            n_class_samples[label] += 1\n",
    "\n",
    "    acc = 100.0 * n_correct / n_samples\n",
    "    print(f'Accuracy of the network: {acc} %')\n",
    "\n",
    "    for i in range(10):\n",
    "        acc = 100.0 * n_class_correct[i] / n_class_samples[i]\n",
    "        print(f'Accuracy of {classes[i]}: {acc} %')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bf1c55-6b0e-47e9-a052-881d9c631c80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
